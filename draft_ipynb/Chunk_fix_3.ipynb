{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd3370a8-7012-461e-9386-eaab6ce42ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = ['S/_p(_eve,_carol)/0->aO/p', 'S/_p(_eve,_alice)/0->aΚ/p', \n",
    "            'S/_follow(_kalin,_mckirby)/0->stu', 'S/_admire(_kalin,_mckirby)/0->jkl',\n",
    "            'S/_judge(_affordance,_autopoiesis)/0->bnm', 'S/_judge(_koko,_autopoiesis)/0->l',\n",
    "            'S/_like(_ibuki,_john)/0->def', 'S/_like(_ibuki,_luce)/0->drf'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26109846-0062-45c1-9ba2-d2b2e53ff778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = ['S/_like(_eve,_carol)/0->abc', 'S/_like(_eve,_alice)/0->adc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f997116a-9105-43ad-b0da-5f6aa488ce03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = ['S/_like(_eve,_carol)/0->abc', 'S/_like(_eve,_alice)/0->def']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33467f73-ea08-41d6-a766-b6dc047daf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = ['S/_p(_eve,_carol)/0->aO/pb', 'S/_p(_eve,_alice)/0->aO/pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfff43eb-da20-4630-a66b-e414aa283de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 9/26 着手のバグ\n",
    "\n",
    "rule_set = ['S/_p(_bob,_david)/0->jQ/p', 'S/_p(_bob,_alice)/0->jU/p'] # 問題のrule_set\n",
    "\n",
    "# rule_set = ['S/_p(_bob,_david)/0->aO/p', 'S/_p(_bob,_alice)/0->aΚ/p'] # オリジナル\n",
    "\n",
    "# rule_set = ['S/_p(_eve,_carol)/0->aO/p', 'S/_p(_eve,_alice)/0->aK/p']\n",
    "# rule_set = ['S/_p(_eve,_carol)/0->aO/p', 'S/_p(_eve,_alice)/0->aΚ/p'] # オリジナル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b6dd57-f38e-45b8-84b2-82046020addc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 9/28 着手\n",
    "\n",
    "rule_set = ['S/_kick(_alice,_david)/0->tgtnub_zft_ndmfcaqm', \n",
    "            'S/_meet(_alice,_david)/0->tgtnub_nkf_ndmfcaqm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dc877bef-22ea-43d7-9a84-9ae1034597b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 10/1 着手\n",
    "rule_set = ['S/_p(_bob,_carol)/0->dbbwhenzoihuddΙ/xzeoffq', 'S/_p(_bob,_alice)/0->dbbwhenzoihuddΙ/xzeoffq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2907d9e6-fac0-4175-94fc-fc016934e9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from difflib import SequenceMatcher\n",
    "import string\n",
    "import random\n",
    "\n",
    "# 意味部門\n",
    "\n",
    "def parse_rule(rule):\n",
    "    parts = rule.split('->')\n",
    "    semantic_structure = parts[0].strip()  # 前半部分を意味構造 -> .strip()は空白部分を削除\n",
    "    form = parts[1].strip()  # 後半部分を意味構造\n",
    "    return semantic_structure, form\n",
    "\n",
    "def clustering_rule_set(rule_set):\n",
    "    word_rule_set = []\n",
    "    for rule in rule_set:\n",
    "        semantic_structure, _ = parse_rule(rule)\n",
    "        # ラベルが S 以外で始まるルールを word_rules に分類\n",
    "        if not semantic_structure.startswith(\"S/\"):\n",
    "            word_rule_set.append(rule)\n",
    "    return word_rule_set\n",
    "\n",
    "def set_semantics(rule_set):\n",
    "    semantic_set = []\n",
    "    for a_rule in rule_set:\n",
    "        a_semantics = parse_rule(a_rule)[0]\n",
    "        semantic_set.append(a_semantics)\n",
    "    return semantic_set\n",
    "\n",
    "def split_semantics_ability(semantic_elements):\n",
    "    # 意味表現を単語単位で分割\n",
    "    return re.findall(r'_[a-zA-Z0-9]+|\\(\\w+\\)|[A-Z]+|/[0-9]', semantic_elements)\n",
    "\n",
    "def split_semantics_process(semantic_set):\n",
    "    split_semantic_elements_set = []\n",
    "    for a_semantic_element in semantic_set:\n",
    "        one_of_semantic_set = split_semantics_ability(a_semantic_element)\n",
    "        split_semantic_elements_set.append(one_of_semantic_set)\n",
    "    return split_semantic_elements_set\n",
    "\n",
    "def count_sem_difference_ability(split_sem1, split_sem2):\n",
    "    differences = 0\n",
    "    # 2つのリストのうち短い方の長さに合わせてループを回す\n",
    "    min_length = min(len(split_sem1), len(split_sem2))\n",
    "    for i in range(min_length): \n",
    "        if split_sem1[i] != split_sem2[i]:\n",
    "            differences += 1\n",
    "    # もしリストの長さが異なる場合、その分も差異としてカウントする\n",
    "    differences += abs(len(split_sem1) - len(split_sem2))\n",
    "    return differences\n",
    "\n",
    "def count_sem_difference_process(split_semantic_elements_set):\n",
    "    pairs_with_differences = []\n",
    "    \n",
    "    split_sem_1_2_pairs = list(itertools.combinations(split_semantic_elements_set, 2))\n",
    "    for split_sem1, split_sem2 in split_sem_1_2_pairs:\n",
    "        differences = count_sem_difference_ability(split_sem1, split_sem2)\n",
    "        pairs_with_differences.append((split_sem1, split_sem2, differences))\n",
    "    return pairs_with_differences\n",
    "\n",
    "def detect_sem_pairs_with_only_one_difference(split_semantic_elements_set):\n",
    "    detect_sem_pairs_with_only_one_difference = []\n",
    "    \n",
    "    split_sem_1_2_pairs = list(itertools.combinations(split_semantic_elements_set, 2))\n",
    "    for split_sem1, split_sem2 in split_sem_1_2_pairs:\n",
    "        differences = count_sem_difference_ability(split_sem1, split_sem2)\n",
    "        if differences == 1:\n",
    "            detect_sem_pairs_with_only_one_difference.append((split_sem1, split_sem2, differences))\n",
    "    return detect_sem_pairs_with_only_one_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fd33f794-805e-4238-9d7f-0e8e5da206f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 形式部門\n",
    "\n",
    "def transformed_set_form(can_chunk_semantic_form_pairs, rule_set):\n",
    "    # ルールセットを辞書に変換 : 各ルールの左辺をキー、右辺を値とします\n",
    "    rule_dict = {}\n",
    "    for a_rule in rule_set:\n",
    "        key, value = a_rule.split(\"->\")\n",
    "        rule_dict[key] = value\n",
    "\n",
    "    transformed_form_pairs = []\n",
    "    for left, right, _ in can_chunk_semantic_form_pairs:\n",
    "        left_form = f\"{left[0]}/{left[1]}({left[2]},{left[3]}){left[4]}\"\n",
    "        right_form = f\"{right[0]}/{right[1]}({right[2]},{right[3]}){right[4]}\"\n",
    "        \n",
    "        # 変換された文字列から記号列を取得\n",
    "        left_transformed_form = rule_dict.get(left_form, \"\")\n",
    "        right_transformed_form = rule_dict.get(right_form, \"\")\n",
    "\n",
    "        if left_transformed_form and right_transformed_form:\n",
    "            transformed_form_pairs.append((left_transformed_form, right_transformed_form))\n",
    "\n",
    "    return transformed_form_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09d0207c-6cdf-4227-b9c5-9e2e36b9f18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_form(form):\n",
    "    # 正規表現で「大文字 + スラッシュ + 小文字 (x, y, p)」をひとつの塊として扱う\n",
    "    pattern = re.compile(r'([A-ZΑ-Ω])/(x|y|p)')\n",
    "    \n",
    "    # 見つかった部分を一時的に特殊文字で置き換える\n",
    "    processed_form = form\n",
    "    matches = pattern.findall(form)\n",
    "    replaced_parts = []\n",
    "    \n",
    "    for match in matches:\n",
    "        # 例えば「W/x」を特殊なトークンに置き換える (例: \"_XYP0_\")\n",
    "        part = f\"{match[0]}/{match[1]}\"\n",
    "        token = f\"_XYP{len(replaced_parts)}_\"\n",
    "        processed_form = processed_form.replace(part, token, 1)\n",
    "        replaced_parts.append((token, part))\n",
    "    \n",
    "    return processed_form, replaced_parts\n",
    "\n",
    "def postprocess_form(processed_form, replaced_parts):\n",
    "    # 特殊トークンを元のカテゴリーラベルセットに戻す\n",
    "    for token, original in replaced_parts:\n",
    "        processed_form = processed_form.replace(token, original)\n",
    "    return processed_form\n",
    "\n",
    " # トークンを含む要素を分割して、それぞれを独立した要素にする\n",
    "def split_token_parts(split_form):\n",
    "    final_split_form = []\n",
    "    for part in split_form:\n",
    "        # 特殊トークンを含んでいる部分を見つけたら、その前後の部分と分割\n",
    "        if \"_XYP\" in part:\n",
    "            parts = re.split(r'(_XYP\\d+_)', part)\n",
    "            final_split_form.extend([p for p in parts if p])  # 空要素を除外\n",
    "        else:\n",
    "            final_split_form.append(part)\n",
    "    return final_split_form\n",
    "\n",
    "def split_form_sim_diff_ability(a_form1, a_form2):\n",
    "    # まずはフォームを前処理して、カテゴリーラベルセットを特殊トークンに置き換える\n",
    "    processed_form1, replaced_parts1 = preprocess_form(a_form1)\n",
    "    processed_form2, replaced_parts2 = preprocess_form(a_form2)\n",
    "\n",
    "    # SequenceMatcher で比較\n",
    "    matcher = SequenceMatcher(None, processed_form1, processed_form2)\n",
    "    split_form1 = []\n",
    "    split_form2 = []\n",
    "\n",
    "    # get_opcodes()の出力を確認\n",
    "    opcodes = matcher.get_opcodes()\n",
    "    for tag, i1, i2, j1, j2 in opcodes:\n",
    "        part1 = processed_form1[i1:i2]\n",
    "        part2 = processed_form2[j1:j2]\n",
    "\n",
    "        if tag == 'equal':\n",
    "            split_form1.append(part1)\n",
    "            split_form2.append(part2)\n",
    "        elif tag == 'replace':\n",
    "            split_form1.append(part1)\n",
    "            split_form2.append(part2)\n",
    "        elif tag == 'delete':\n",
    "            split_form1.append(part1)\n",
    "            split_form2.append('')\n",
    "        elif tag == 'insert':\n",
    "            split_form1.append('')\n",
    "            split_form2.append(part2)\n",
    "\n",
    "    # 空の部分集合を削除\n",
    "    split_form1 = [part for part in split_form1 if part]\n",
    "    split_form2 = [part for part in split_form2 if part]\n",
    "    # split_form1 と split_form2 をトークン部分ごとに分割\n",
    "    split_form1 = split_token_parts(split_form1)\n",
    "    split_form2 = split_token_parts(split_form2)\n",
    "\n",
    "    # 結果を元に戻す (トークンを元のカテゴリーラベルセットに復元)\n",
    "    split_form1 = [postprocess_form(part, replaced_parts1) for part in split_form1]\n",
    "    split_form2 = [postprocess_form(part, replaced_parts2) for part in split_form2]\n",
    "\n",
    "    return split_form1, split_form2\n",
    "\n",
    "def split_form_process(transformed_form_pairs):\n",
    "    split_form_pairs = []\n",
    "    for a_form1, a_form2 in transformed_form_pairs:\n",
    "        a_split_form_result = split_form_sim_diff_ability(a_form1, a_form2)\n",
    "        split_form_pairs.append(a_split_form_result)\n",
    "    return split_form_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1c01fb3-28b6-4a85-8081-a5ea30af39a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['dbbwhenzoihudd', 'Ι/x', 'zeoffq'], ['dbbwhenzoihudd', 'Ι/x', 'zeoffq'])]\n"
     ]
    }
   ],
   "source": [
    "word_rule_set = clustering_rule_set(rule_set)\n",
    "for rule in word_rule_set:\n",
    "    rule_set.remove(rule)\n",
    "# 意味側\n",
    "semantic_set = set_semantics(rule_set)\n",
    "split_semantic_elements = split_semantics_process(semantic_set)\n",
    "can_chunk_semantic_form_pairs = detect_sem_pairs_with_only_one_difference(split_semantic_elements)\n",
    "transformed_form_pairs = transformed_set_form(can_chunk_semantic_form_pairs, rule_set)\n",
    "\n",
    "split_form_pairs = split_form_process(transformed_form_pairs)\n",
    "print(split_form_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc737c39-f6d5-490d-a379-800bc2e33baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_forms_by_index_ability(a_form1_as_list, a_form2_as_list): \n",
    "    if len(a_form1_as_list) != len(a_form2_as_list):\n",
    "        return \"2\"\n",
    "    \n",
    "    comparison_form_result_by_index = []\n",
    "    \n",
    "    for index in range(len(a_form1_as_list)):\n",
    "        if a_form1_as_list[index] == a_form2_as_list[index]:\n",
    "            comparison_form_result_by_index.append('0')\n",
    "        else:\n",
    "            comparison_form_result_by_index.append('1')\n",
    "            \n",
    "    return ''.join(comparison_form_result_by_index)\n",
    "\n",
    "def compare_forms_by_index_process(split_form_pairs):\n",
    "    compare_form_pair_results = []\n",
    "    for a_form1_as_list, a_form2_as_list in split_form_pairs:\n",
    "        a_compare_form_pair_result = compare_forms_by_index_ability(a_form1_as_list, a_form2_as_list)\n",
    "        # compare_form_pair_results.append(a_compare_form_pair_result)\n",
    "        compare_form_pair_results.append((a_compare_form_pair_result, a_form1_as_list, a_form2_as_list))  # 各ペアを結果と一緒に保存\n",
    "    return compare_form_pair_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4b0f01bb-c5df-41b0-9464-f8ea309fdc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('000', ['dbbwhenzoihudd', 'Ι/x', 'zeoffq'], ['dbbwhenzoihudd', 'Ι/x', 'zeoffq'])]\n"
     ]
    }
   ],
   "source": [
    "compare_form_pair_results = compare_forms_by_index_process(split_form_pairs)\n",
    "print(compare_form_pair_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b206d80f-dbab-4bc9-81fb-271fae070203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contains_uppercase_greek_or_latin(character):\n",
    "    # ラテン文字の大文字とギリシア文字の大文字に対するチェック\n",
    "    return character.isupper() or ('Α' <= character <= 'Ω')\n",
    "\n",
    "# def check_for_uppercase_warning(compare_form_pair_results):\n",
    "#     for result, form1_list, form2_list in compare_form_pair_results:\n",
    "#         for i, char in enumerate(result):\n",
    "#             if char == '1':  # 差異部分を検出\n",
    "#                 # form1_list[i] と form2_list[i] に大文字のアルファベットまたはギリシア文字が含まれているか確認\n",
    "#                 if contains_uppercase_greek_or_latin(form1_list[i]) or contains_uppercase_greek_or_latin(form2_list[i]):\n",
    "#                     print(f\"Warning: Uppercase letter or Greek letter found at index {i} in form pair.\")\n",
    "#                     print(f\"Form1: {form1_list[i]}, Form2: {form2_list[i]}\")\n",
    "\n",
    "def check_for_uppercase_warning(compare_form_pair_results):\n",
    "    for idx, (result, form1_list, form2_list) in enumerate(compare_form_pair_results):\n",
    "        for i, char in enumerate(result):\n",
    "            if char == '1':  # 差異部分を検出\n",
    "                # form1_list[i] と form2_list[i] に大文字のアルファベットまたはギリシア文字が含まれているか確認\n",
    "                # if contains_uppercase_greek_or_latin(form1_list[i]) or contains_uppercase_greek_or_latin(form2_list[i]):\n",
    "                if any(contains_uppercase_greek_or_latin(c) for c in form1_list[i]) or any(contains_uppercase_greek_or_latin(c) for c in form2_list[i]):\n",
    "                    # print(f\"Warning: Uppercase letter or Greek letter found at index {i} in form pair.\")\n",
    "                    # print(f\"Form1: {form1_list[i]}, Form2: {form2_list[i]}\")\n",
    "                    \n",
    "                    # ここでa_compare_form_pair_result（つまり result）を'2'に変更\n",
    "                    compare_form_pair_results[idx] = ('2', form1_list, form2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b86cc13c-76c3-4a85-aaf1-664312e8fe1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_filtered_indices(compare_form_pair_results):\n",
    "    filtered_indices_set = []\n",
    "    for index, element in enumerate(compare_form_pair_results):\n",
    "        if (\n",
    "            2 <= len(element) <= 3 and\n",
    "            element.count('1') < 2 and\n",
    "            # '00' not in element and # 同じものが並んでいる　    ---------------------------------ここの条件を外した\n",
    "            # '11' not in element and # 異なるものが並んでいる　   ---------------------------------ここの条件を外した\n",
    "            element != '2' and\n",
    "            not all(c == '0' for c in element)  # すべてが0である場合は対象外\n",
    "        ):\n",
    "            filtered_indices_set.append(index)\n",
    "    return filtered_indices_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0446fb0c-c8cd-41fb-aef0-3f64b71159ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 比較結果を出力するための処理\n",
    "compare_form_pair_results = compare_forms_by_index_process(split_form_pairs)\n",
    "# print(compare_form_pair_results)\n",
    "\n",
    "# 差異部分に大文字のアルファベットやギリシア文字が含まれているかをチェックして警告を表示し、a_compare_form_pair_resultを'2'に変更\n",
    "check_for_uppercase_warning(compare_form_pair_results)\n",
    "\n",
    "# 変更後の結果を確認\n",
    "# print(compare_form_pair_results)\n",
    "\n",
    "filtered_indices_set = get_filtered_indices([result for result, _, _ in compare_form_pair_results])\n",
    "print(filtered_indices_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fcf1ba3-f926-48a2-ac78-fcb20debb790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def final_transformed_set_form(can_chunk_semantic_form_pairs, rule_set, filtered_indices_set):\n",
    "    rule_dict = {}\n",
    "    for a_rule in rule_set:\n",
    "        key, value = a_rule.split(\"->\")\n",
    "        rule_dict[key] = value\n",
    "\n",
    "    all_transformed_form_pairs = []\n",
    "    for index in filtered_indices_set:\n",
    "        transformed_form_pairs = []\n",
    "        selected_pair = can_chunk_semantic_form_pairs[index]\n",
    "\n",
    "        left, right, _ = selected_pair\n",
    "        left_form = f\"{left[0]}/{left[1]}({left[2]},{left[3]}){left[4]}\"\n",
    "        right_form = f\"{right[0]}/{right[1]}({right[2]},{right[3]}){right[4]}\"\n",
    "\n",
    "        left_transformed_form = rule_dict.get(left_form, \"\")\n",
    "        right_transformed_form = rule_dict.get(right_form, \"\")\n",
    "\n",
    "        if left_transformed_form:\n",
    "            transformed_form_pairs.append(f\"{left_form}->{left_transformed_form}\")\n",
    "        if right_transformed_form:\n",
    "            transformed_form_pairs.append(f\"{right_form}->{right_transformed_form}\")\n",
    "\n",
    "        all_transformed_form_pairs.append(transformed_form_pairs)\n",
    "\n",
    "    return all_transformed_form_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0dcbfe3-5d15-4fdf-849f-e9ae04c446ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S/_p(_bob,_carol)/0->quF/p', 'S/_p(_bob,_alice)/0->mdF/p']]\n"
     ]
    }
   ],
   "source": [
    "all_transformed_form_pairs = final_transformed_set_form(can_chunk_semantic_form_pairs, rule_set, filtered_indices_set)\n",
    "print(all_transformed_form_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f3778-5fa9-4f52-b55c-127608d71f35",
   "metadata": {},
   "source": [
    "この時点で，chunk できるペアの検出が終わる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ed47456-99ef-4a07-afd4-2f620c9000b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_only_sem_chunk_pair(can_chunk_rule_set):\n",
    "    transform_only_sem_chunk_pair_sets = []\n",
    "\n",
    "    for a_can_chunk_rule in can_chunk_rule_set:\n",
    "        transformed_a_can_chunk_rule_pair = []\n",
    "        for an_element_of_a_can_chunk_rule in a_can_chunk_rule:\n",
    "            semantic_structure, _ = parse_rule(an_element_of_a_can_chunk_rule)\n",
    "            transformed_a_can_chunk_rule_pair.append(semantic_structure)\n",
    "        transform_only_sem_chunk_pair_sets.append(transformed_a_can_chunk_rule_pair)\n",
    "    \n",
    "    return transform_only_sem_chunk_pair_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43ae0a69-99df-46ee-8494-a7c12a0f5859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S/_p(_bob,_carol)/0', 'S/_p(_bob,_alice)/0']]\n"
     ]
    }
   ],
   "source": [
    "transform_only_sem_chunk_pair_sets =  transform_only_sem_chunk_pair(all_transformed_form_pairs)\n",
    "print(transform_only_sem_chunk_pair_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62355045-41d9-4948-b398-51d2cf205ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_sem_pairs_for_chunk(sem_chunk_pair_sets):\n",
    "    split_sem_pairs = []\n",
    "    for pair in sem_chunk_pair_sets:\n",
    "        split_pair = []\n",
    "        for semantic_element in pair:\n",
    "            split_element = split_semantics_ability(semantic_element)\n",
    "            split_pair.append(split_element)\n",
    "        split_sem_pairs.append(split_pair)\n",
    "    return split_sem_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "758f3878-f3a1-4c85-ba74-dc52128852df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['S', '_p', '_bob', '_carol', '/0'], ['S', '_p', '_bob', '_alice', '/0']]]\n"
     ]
    }
   ],
   "source": [
    "split_sem_pairs = split_sem_pairs_for_chunk(transform_only_sem_chunk_pair_sets)\n",
    "print(split_sem_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8baff7a-84ac-4f42-9c61-dead9b2ef990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_index_sem_difference_ability(split_sem1, split_sem2):\n",
    "    differing_indices = []\n",
    "    for i in range(len(split_sem1)): \n",
    "        if split_sem1[i] != split_sem2[i]:\n",
    "            differing_indices.append(i)\n",
    "    return differing_indices\n",
    "\n",
    "def detect_index_sem_difference_process(split_sem_pairs):\n",
    "    index_sem_difference_set = []\n",
    "    for pair in split_sem_pairs:\n",
    "        if len(pair) < 2:\n",
    "            continue  # もしペアが2つの要素を持たない場合、スキップする\n",
    "        differing_indices = detect_index_sem_difference_ability(pair[0], pair[1])\n",
    "        index_sem_difference_set.append(differing_indices)\n",
    "    return index_sem_difference_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92515a32-10bc-4022-8c1b-ab0c54a35be3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]]\n"
     ]
    }
   ],
   "source": [
    "index_sem_difference_sets = detect_index_sem_difference_process(split_sem_pairs)\n",
    "print(index_sem_difference_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b740b26c-25ca-40fe-be9c-e43d87bb3072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_only_form_chunk_pair(can_chunk_rule_set):\n",
    "    transform_only_form_chunk_pair_sets = []\n",
    "\n",
    "    for a_can_chunk_rule in can_chunk_rule_set:\n",
    "        transformed_a_can_chunk_rule_pair = []\n",
    "        for an_element_of_a_can_chunk_rule in a_can_chunk_rule:\n",
    "            _, form = parse_rule(an_element_of_a_can_chunk_rule)\n",
    "            transformed_a_can_chunk_rule_pair.append(form)\n",
    "        transform_only_form_chunk_pair_sets.append(transformed_a_can_chunk_rule_pair)\n",
    "    \n",
    "    return transform_only_form_chunk_pair_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c61c6e53-8aba-4804-b509-26654b095482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['quF/p', 'mdF/p']]\n"
     ]
    }
   ],
   "source": [
    "transform_only_form_chunk_pair_sets = transform_only_form_chunk_pair(all_transformed_form_pairs)\n",
    "print(transform_only_form_chunk_pair_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b33ba61-d5e7-4c12-b0e6-1263ff648b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_form_process(form_chunk_pair_sets):\n",
    "    split_form_pairs = []\n",
    "    for pair in form_chunk_pair_sets:\n",
    "        if len(pair) != 2:\n",
    "            continue  # 要素数が2でない場合はスキップ\n",
    "        a_form1, a_form2 = pair\n",
    "        a_split_form_result = split_form_sim_diff_ability(a_form1, a_form2)\n",
    "        split_form_pairs.append(a_split_form_result)\n",
    "    return split_form_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fea1d88-5491-45a3-85c2-ea95565edc7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['qu', 'F/p'], ['md', 'F/p'])]\n"
     ]
    }
   ],
   "source": [
    "split_form_pairs = split_form_process(transform_only_form_chunk_pair_sets)\n",
    "print(split_form_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ee1854a-f347-4392-90ae-fb23c4c3d854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_index_form_difference_ability(a_form1_as_list, a_form2_as_list): \n",
    "    if len(a_form1_as_list) != len(a_form2_as_list):\n",
    "        return \"長さが異なります\"\n",
    "    \n",
    "    differing_indices = []\n",
    "    \n",
    "    for index in range(len(a_form1_as_list)):\n",
    "        if a_form1_as_list[index] != a_form2_as_list[index]:\n",
    "            differing_indices.append(index)\n",
    "    \n",
    "    return differing_indices\n",
    "\n",
    "def detect_index_form_difference_process(split_form_pairs):\n",
    "    index_form_difference_sets = []\n",
    "    for a_form1_as_list, a_form2_as_list in split_form_pairs:\n",
    "        differing_indices = detect_index_form_difference_ability(a_form1_as_list, a_form2_as_list)\n",
    "        index_form_difference_sets.append(differing_indices)\n",
    "    \n",
    "    return index_form_difference_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4624038e-414f-4de5-a339-ae7644ab296b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "index_form_difference_sets = detect_index_form_difference_process(split_form_pairs)\n",
    "print(index_form_difference_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5bc69cf-0279-45cf-8e1f-b654392c7e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_random_label(used_labels):\n",
    "    # 英字の大文字とギリシャ文字の大文字を定義\n",
    "    greek_uppercase = [\n",
    "        'Α', 'Β', 'Γ', 'Δ', 'Ε', 'Ζ', 'Η', 'Θ', 'Ι', 'Κ', 'Λ', 'Μ',\n",
    "        'Ν', 'Ξ', 'Ο', 'Π', 'Σ', 'Τ', 'Φ', 'Ψ', 'Ω'\n",
    "    ]\n",
    "    \n",
    "    # 英語のアルファベット大文字とギリシャ文字の大文字を組み合わせる\n",
    "    all_labels = list(string.ascii_uppercase) + greek_uppercase\n",
    "    \n",
    "    # 除外する文字\n",
    "    excluded_labels = {'S', 'X', 'Y', 'P'}\n",
    "    \n",
    "    # 使用可能なラベルセットを計算\n",
    "    available_labels = list(set(all_labels) - used_labels - excluded_labels)\n",
    "    # デバッグ: 使用可能なラベルの内容を表示\n",
    "    print(f\"Available labels: {available_labels}\")\n",
    "    \n",
    "    # ランダムに1つのラベルを選択\n",
    "    label = random.choice(available_labels)\n",
    "    \n",
    "    # 使用済みのラベルとして追加\n",
    "    used_labels.add(label)\n",
    "    \n",
    "    return label\n",
    "\n",
    "def apply_existing_labels_for_type2_chunk(used_labels, split_form_pairs):\n",
    "    for split_form_pair in split_form_pairs:\n",
    "        label_count = {}\n",
    "        for split_form in split_form_pair:\n",
    "            for element in split_form:\n",
    "                if '/' in element:\n",
    "                    label, _ = element.split('/')\n",
    "                    label_count[label] = label_count.get(label, 0) + 1\n",
    "        \n",
    "        # デバッグ: 各ラベルのカウント状況を表示\n",
    "        print(f\"Label count: {label_count}\")\n",
    "        \n",
    "        for label, count in label_count.items():\n",
    "            if count >= 2:\n",
    "                used_labels.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bdf5c40-bd76-43c3-9f19-df9a01a0f847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_sem_pairs = [[['S', '_p', '_bob', '_carol', '/0'], ['S', '_p', '_bob', '_alice', '/0']]]\n",
    "split_form_pairs = [(['dbbwhenzoihudd', 'Ι/x', 'zeoffq'], ['dbbwhenzoihudd', 'Ι/x', 'zeoffq'])]\n",
    "index_sem_difference_sets = [[3]]\n",
    "index_form_difference_sets = [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c088acdb-c040-4ade-be96-9d83c380f2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_completed_to_generate_scheme_rules_and_word_rules(\n",
    "    split_sem_pairs,\n",
    "    split_form_pairs,\n",
    "    index_sem_difference_sets,\n",
    "    index_form_difference_sets\n",
    "):\n",
    "    chunk_completed_pairs = []\n",
    "    used_labels = set()\n",
    "    # デバッグ: ラベル適用前の状況を表示\n",
    "    print(f\"Initial used_labels: {used_labels}\")\n",
    "    \n",
    "    apply_existing_labels_for_type2_chunk(used_labels, split_form_pairs)\n",
    "    # デバッグ: ラベル適用後の状況を表示\n",
    "    print(f\"Used labels after applying existing labels: {used_labels}\")\n",
    "\n",
    "    index_to_var = {1: '_p', 2: '_x', 3: '_y'}\n",
    "\n",
    "    for sem_pair, form_pair, sem_diff, form_diff in zip(\n",
    "        split_sem_pairs,\n",
    "        split_form_pairs,\n",
    "        index_sem_difference_sets,\n",
    "        index_form_difference_sets\n",
    "    ):\n",
    "       \n",
    "        existing_label = None\n",
    "        for form_elements in form_pair:\n",
    "            for element in form_elements:\n",
    "                if '/' in element:\n",
    "                    label, _ = element.split('/')\n",
    "                    # 使用済みラベル (2回使われているラベル) を無視\n",
    "                    if label not in used_labels:\n",
    "                        existing_label = label\n",
    "                        break\n",
    "            if existing_label is not None:\n",
    "                break\n",
    "        \n",
    "        # デバッグ: 既存ラベルの確認\n",
    "        print(f\"Existing label: {existing_label}\")\n",
    "\n",
    "        if existing_label is not None:\n",
    "            label = existing_label\n",
    "            used_labels.add(label)  # 既存ラベルを使用済みラベルに追加\n",
    "        else:\n",
    "            label = generate_random_label(used_labels)\n",
    "        \n",
    "        # デバッグ: 使用されるラベルの確認\n",
    "        print(f\"Chosen label: {label}\")\n",
    "\n",
    "        index = sem_diff[0]\n",
    "        var = index_to_var.get(index, '_x')\n",
    "\n",
    "        sem_of_scheme_rule = sem_pair[0][:]\n",
    "        sem_of_scheme_rule[index] = var\n",
    "\n",
    "        form_of_scheme_rule = form_pair[0][:]\n",
    "        form_of_scheme_rule[form_diff[0]] = f'{label}/{var[1]}'\n",
    "        \n",
    "        scheme_rule = [sem_of_scheme_rule, form_of_scheme_rule]\n",
    "        \n",
    "        sem_of_word_rule_1 = [f'{label}', sem_pair[0][index]]\n",
    "        sem_of_word_rule_2 = [f'{label}', sem_pair[1][index]]\n",
    "\n",
    "        form_of_word_rule_1 = [form_pair[0][form_diff[0]]]\n",
    "        form_of_word_rule_2 = [form_pair[1][form_diff[0]]]\n",
    "\n",
    "        word_rule_1 = [sem_of_word_rule_1, form_of_word_rule_1]\n",
    "        word_rule_2 = [sem_of_word_rule_2, form_of_word_rule_2]\n",
    "\n",
    "        word_rules = []\n",
    "        unwanted_vars = ['_p', '_x', '_y']\n",
    "        for word_rule in [word_rule_1, word_rule_2]:\n",
    "            if not any(var in word_rule[0] for var in unwanted_vars):\n",
    "                word_rules.append(word_rule)\n",
    "\n",
    "        chunk_completed_pairs.append((scheme_rule, *word_rules))\n",
    "        \n",
    "    return chunk_completed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8600dfb4-c41a-468b-a48a-fbf2ac61888d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial used_labels: set()\n",
      "Label count: {'Ι': 2}\n",
      "Used labels after applying existing labels: {'Ι'}\n",
      "Existing label: None\n",
      "Available labels: ['A', 'K', 'L', 'R', 'Η', 'O', 'M', 'G', 'T', 'Ξ', 'Λ', 'Ω', 'Z', 'Κ', 'C', 'U', 'I', 'Β', 'Σ', 'Γ', 'Ο', 'D', 'B', 'W', 'Θ', 'Φ', 'H', 'Ε', 'Ζ', 'Τ', 'J', 'V', 'F', 'E', 'N', 'Δ', 'Q', 'Μ', 'Ψ', 'Α', 'Π', 'Ν']\n",
      "Chosen label: M\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chunk_completed_pairs \u001b[38;5;241m=\u001b[39m chunk_completed_to_generate_scheme_rules_and_word_rules(\n\u001b[1;32m      2\u001b[0m     split_sem_pairs,\n\u001b[1;32m      3\u001b[0m     split_form_pairs,\n\u001b[1;32m      4\u001b[0m     index_sem_difference_sets,\n\u001b[1;32m      5\u001b[0m     index_form_difference_sets\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunk_completed_pairs)\n",
      "Cell \u001b[0;32mIn[41], line 56\u001b[0m, in \u001b[0;36mchunk_completed_to_generate_scheme_rules_and_word_rules\u001b[0;34m(split_sem_pairs, split_form_pairs, index_sem_difference_sets, index_form_difference_sets)\u001b[0m\n\u001b[1;32m     53\u001b[0m sem_of_scheme_rule[index] \u001b[38;5;241m=\u001b[39m var\n\u001b[1;32m     55\u001b[0m form_of_scheme_rule \u001b[38;5;241m=\u001b[39m form_pair[\u001b[38;5;241m0\u001b[39m][:]\n\u001b[0;32m---> 56\u001b[0m form_of_scheme_rule[form_diff[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     58\u001b[0m scheme_rule \u001b[38;5;241m=\u001b[39m [sem_of_scheme_rule, form_of_scheme_rule]\n\u001b[1;32m     60\u001b[0m sem_of_word_rule_1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, sem_pair[\u001b[38;5;241m0\u001b[39m][index]]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "chunk_completed_pairs = chunk_completed_to_generate_scheme_rules_and_word_rules(\n",
    "    split_sem_pairs,\n",
    "    split_form_pairs,\n",
    "    index_sem_difference_sets,\n",
    "    index_form_difference_sets\n",
    ")\n",
    "print(chunk_completed_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d2c8ac9-d3d0-4cbd-97d5-0b1ca29e1ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_learning(rule_set):\n",
    "    \n",
    "    word_rule_set = clustering_rule_set(rule_set)\n",
    "    for rule in word_rule_set:\n",
    "        rule_set.remove(rule)\n",
    "    # 意味側\n",
    "    semantic_set = set_semantics(rule_set)\n",
    "    split_semantic_elements = split_semantics_process(semantic_set)\n",
    "    can_chunk_semantic_form_pairs = detect_sem_pairs_with_only_one_difference(split_semantic_elements)\n",
    "    \n",
    "    # 形式側\n",
    "    transformed_form_pairs = transformed_set_form(can_chunk_semantic_form_pairs, rule_set)\n",
    "    split_form_pairs = split_form_process(transformed_form_pairs)\n",
    "    form_pair_results = compare_forms_by_index_process(split_form_pairs)\n",
    "    result_indices = get_filtered_indices(form_pair_results)\n",
    "    can_chunk_rule_set = final_transformed_set_form(can_chunk_semantic_form_pairs, rule_set, result_indices)\n",
    "\n",
    "    # 各ステップの処理を実行\n",
    "    transform_only_sem_chunk_pairs = transform_only_sem_chunk_pair(can_chunk_rule_set)\n",
    "    \n",
    "    # 変数名を変更して初期化の確認を追加\n",
    "    split_sem_pairs = split_sem_pairs_for_chunk(transform_only_sem_chunk_pairs) if transform_only_sem_chunk_pairs else []\n",
    "\n",
    "    index_sem_difference_sets = detect_index_sem_difference_process(split_sem_pairs)\n",
    "\n",
    "    transform_only_form_chunk_pairs = transform_only_form_chunk_pair(can_chunk_rule_set)\n",
    "    split_form_pairs_for_chunk = split_form_process(transform_only_form_chunk_pairs)\n",
    "    index_form_difference_sets = detect_index_form_difference_process(split_form_pairs_for_chunk)\n",
    "\n",
    "    chunk_completed_pairs = chunk_completed_to_generate_scheme_rules_and_word_rules(\n",
    "        split_sem_pairs,\n",
    "        split_form_pairs_for_chunk,\n",
    "        index_sem_difference_sets,\n",
    "        index_form_difference_sets\n",
    "    )\n",
    "\n",
    "    # ルールセット全体をコピーして保持\n",
    "    remaining_rules = rule_set[:]\n",
    "\n",
    "    # can_chunk_rule_set に含まれるルールを used_rules に追加\n",
    "    used_rules = []\n",
    "    for rule_pair in can_chunk_rule_set:\n",
    "        for rule in rule_pair:\n",
    "            # 元の rule_set にあるかを確認し、あれば used_rules に追加\n",
    "            for original_rule in rule_set:\n",
    "                if rule in original_rule:\n",
    "                    used_rules.append(original_rule)\n",
    "\n",
    "    # 未使用のルールを特定\n",
    "    unapplied_rules = [rule for rule in remaining_rules if rule not in used_rules]\n",
    "\n",
    "    chunked_rules = []\n",
    "\n",
    "    # chunk_completed_to_generate_scheme_rules_and_word_rules_pairs のルールを整形して chunked_rules に追加\n",
    "    for scheme_rule, *word_rules in chunk_completed_pairs:\n",
    "        # スキーマルールの整形\n",
    "        sem_scheme_rule = scheme_rule[0]\n",
    "        \n",
    "        # 2番目と3番目の要素をまとめて括弧で囲む\n",
    "        if len(sem_scheme_rule) >= 4:\n",
    "            combined_element = f\"({sem_scheme_rule[2]},{sem_scheme_rule[3]})\"\n",
    "            sem_scheme_rule = sem_scheme_rule[:2] + [combined_element] + sem_scheme_rule[4:]\n",
    "\n",
    "        # スキーマルールを文字列に結合\n",
    "        sem_scheme_rule = f\"{sem_scheme_rule[0]}/\" + \"\".join(sem_scheme_rule[1:])\n",
    "        form_scheme_rule = \"\".join(scheme_rule[1])\n",
    "\n",
    "        chunked_rules.append(f\"{sem_scheme_rule}->{form_scheme_rule}\")\n",
    "        \n",
    "        # 単語ルールの整形\n",
    "        for word_rule in word_rules:\n",
    "            sem_word_rule = \"/\".join(word_rule[0])\n",
    "            form_word_rule = \"\".join(word_rule[1])\n",
    "            chunked_rules.append(f\"{sem_word_rule}->{form_word_rule}\")\n",
    "\n",
    "    # unapplied_rules を chunked_rules に追加\n",
    "    # word_rule_set を chunked_rules に追加\n",
    "    chunked_rules.extend(unapplied_rules)\n",
    "    chunked_rules.extend(word_rule_set)\n",
    "    \n",
    "    return chunked_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25481a99-ee6e-4bcb-b6ff-9ee20d5caeca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial used_labels: set()\n",
      "Label count: {'F': 2}\n",
      "Used labels after applying existing labels: {'F'}\n",
      "Existing label: None\n",
      "Available labels: ['A', 'K', 'L', 'R', 'Η', 'O', 'M', 'G', 'T', 'Ξ', 'Λ', 'Ω', 'Z', 'Κ', 'Ι', 'C', 'U', 'I', 'Β', 'Σ', 'Γ', 'Ο', 'D', 'B', 'W', 'Θ', 'Φ', 'H', 'Ε', 'Ζ', 'Τ', 'J', 'V', 'N', 'E', 'Δ', 'Q', 'Μ', 'Ψ', 'Α', 'Π', 'Ν']\n",
      "Chosen label: T\n",
      "['S/_p(_bob,_y)/0->T/yF/p', 'T/_carol->qu', 'T/_alice->md']\n"
     ]
    }
   ],
   "source": [
    "chunked_rules = chunk_learning(rule_set)\n",
    "print(chunked_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
