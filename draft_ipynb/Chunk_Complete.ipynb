{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f8d107-45d0-48c0-8eff-d5c493feb12d",
   "metadata": {},
   "source": [
    "Chunk完成版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25691a9b-3e64-4fb5-9635-45438c785568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = [\n",
    "    \"S/_judge(_eve,_quolia)/0 -> abc\", \n",
    "    \"S/_follow(_eve,_david)/0 -> adc\",\n",
    "    \"S/_follow(_alice,_david)/0 -> rtu\",\n",
    "    \"S/_follow(_alice,_jazz)/0 -> rtw\",\n",
    "    \"S/_kick(_david,_y)/0 -> A/yl\", \n",
    "    \"A/_alice -> eair\", \n",
    "    \"A/_bob -> fngkwu\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505a4bd8-7780-43a5-872c-54a3a2b06ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = [\n",
    "    \"S/_judge(_eve,_quolia)/0 -> abc\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d21d21-52c1-4536-baf7-c68da633cfc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 449\u001b[0m\n\u001b[1;32m    445\u001b[0m can_chunk_semantic_form_pairs \u001b[38;5;241m=\u001b[39m detect_sem_pairs_with_only_one_difference(split_semantic_elements)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# print(can_chunk_semantic_form_pairs)\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# 形式側\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m transformed_form_pairs \u001b[38;5;241m=\u001b[39m transformed_set_form(can_chunk_semantic_form_pairs, rule_set)\n\u001b[1;32m    450\u001b[0m split_form_pairs \u001b[38;5;241m=\u001b[39m split_form_process(transformed_form_pairs)\n\u001b[1;32m    451\u001b[0m form_pair_results \u001b[38;5;241m=\u001b[39m compare_forms_by_index_process(split_form_pairs)\n",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mtransformed_set_form\u001b[0;34m(can_chunk_semantic_form_pairs, rule_set)\u001b[0m\n\u001b[1;32m    101\u001b[0m transformed_form_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m left, right, _ \u001b[38;5;129;01min\u001b[39;00m can_chunk_semantic_form_pairs:\n\u001b[0;32m--> 103\u001b[0m     left_form \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     right_form \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mright[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mright[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mright[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mright[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mright[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# 変換された文字列から記号列を取得\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import itertools\n",
    "from difflib import SequenceMatcher\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "# 意味部門\n",
    "\n",
    "def parse_rule(rule):\n",
    "    # -> の前後で分割するだけ\n",
    "    parts = rule.split('->')\n",
    "    semantic_structure = parts[0].strip() # 前半部分を意味構造　　 # .strip()は空白部分を削除\n",
    "    form = parts[1].strip()  # 後半部分を意味構造\n",
    "    return semantic_structure, form\n",
    "\n",
    "def set_semantics(rule_set):\n",
    "    semantic_set = []\n",
    "    for a_rule in rule_set:\n",
    "        a_semantics = parse_rule(a_rule)[0]\n",
    "        semantic_set.append(a_semantics)\n",
    "    return semantic_set\n",
    "\n",
    "def split_semantics_ability(semantic_elements):\n",
    "    # 意味表現を単語単位で分割\n",
    "    return re.findall(r'_[a-zA-Z0-9]+|\\(\\w+\\)|[A-Z]+|/[0-9]', semantic_elements)\n",
    "\n",
    "def split_semantics_process(semantic_set):\n",
    "    split_semantic_elements_set = []\n",
    "    for a_semantic_element in semantic_set:\n",
    "        one_of_semantic_set = split_semantics_ability(a_semantic_element)\n",
    "        split_semantic_elements_set.append(one_of_semantic_set)\n",
    "    return split_semantic_elements_set\n",
    "\n",
    "\"\"\"def count_sem_difference_ability(split_sem1, split_sem2):\n",
    "    differences = 0\n",
    "    # len(split_sem1)によって，['S', '_judge', '_eve','_carol)', '/0']が5つの要素であることを確かめている\n",
    "    for i in range(len(split_sem1)): \n",
    "        if split_sem1[i] != split_sem2[i]:\n",
    "            differences += 1\n",
    "    return differences\"\"\"\n",
    "\n",
    "\n",
    "def count_sem_difference_ability(split_sem1, split_sem2):\n",
    "    differences = 0\n",
    "    # 2つのリストのうち短い方の長さに合わせてループを回す\n",
    "    min_length = min(len(split_sem1), len(split_sem2))\n",
    "    for i in range(min_length): \n",
    "        if split_sem1[i] != split_sem2[i]:\n",
    "            differences += 1\n",
    "    # もしリストの長さが異なる場合、その分も差異としてカウントする\n",
    "    differences += abs(len(split_sem1) - len(split_sem2))\n",
    "    return differences\n",
    "\n",
    "\n",
    "\n",
    "def count_sem_difference_process(split_semantic_elements_set):\n",
    "    pairs_with_differences = []\n",
    "    \n",
    "    split_sem_1_2_pairs = list(itertools.combinations(split_semantic_elements_set, 2))\n",
    "    for split_sem1, split_sem2 in split_sem_1_2_pairs:\n",
    "        differences = count_sem_difference_ability(split_sem1, split_sem2)\n",
    "        pairs_with_differences.append((split_sem1, split_sem2, differences))\n",
    "    return pairs_with_differences\n",
    "\n",
    "def detect_sem_pairs_with_only_one_difference(split_semantic_elements_set):\n",
    "    detect_sem_pairs_with_only_one_difference = []\n",
    "    \n",
    "    split_sem_1_2_pairs = list(itertools.combinations(split_semantic_elements_set, 2))\n",
    "    for split_sem1, split_sem2 in split_sem_1_2_pairs:\n",
    "        differences = count_sem_difference_ability(split_sem1, split_sem2)\n",
    "        if differences == 1:\n",
    "            detect_sem_pairs_with_only_one_difference.append((split_sem1, split_sem2, differences))\n",
    "    return detect_sem_pairs_with_only_one_difference\n",
    "\n",
    "\"\"\"\n",
    "semantic_set = set_semantics(rule_set)\n",
    "split_semantic_elements = split_semantics_process(semantic_set)\n",
    "can_chunk_semantic_form_pairs = detect_sem_pairs_with_only_one_difference(split_semantic_elements)\n",
    "\n",
    "# print(can_chunk_semantic_form_pairs)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 形式部門\n",
    "\n",
    "# とりま，分割\n",
    "def parse_rule(rule):\n",
    "    parts = rule.split('->')\n",
    "    semantic_structure = parts[0].strip()\n",
    "    form = parts[1].strip()\n",
    "    return semantic_structure, form\n",
    "\n",
    "def transformed_set_form(can_chunk_semantic_form_pairs, rule_set):\n",
    "    # ルールセットを辞書に変換 : 各ルールの左辺をキー， 右辺を値とします.\n",
    "    rule_dict = {}\n",
    "    for a_rule in rule_set:\n",
    "        key, value = a_rule.split(\" -> \")\n",
    "        rule_dict[key] = value\n",
    "\n",
    "    transformed_form_pairs = []\n",
    "    for left, right, _ in can_chunk_semantic_form_pairs:\n",
    "        left_form = f\"{left[0]}/{left[1]}({left[2]},{left[3]}){left[4]}\"\n",
    "        right_form = f\"{right[0]}/{right[1]}({right[2]},{right[3]}){right[4]}\"\n",
    "        \n",
    "        # 変換された文字列から記号列を取得\n",
    "        left_transformed_form = rule_dict.get(left_form, \"\")\n",
    "        right_transformed_form = rule_dict.get(right_form, \"\")\n",
    "\n",
    "        if left_transformed_form and right_transformed_form:\n",
    "            transformed_form_pairs.append((left_transformed_form, right_transformed_form))\n",
    "\n",
    "    return transformed_form_pairs\n",
    "\n",
    "def split_form_sim_diff_ability(a_form1, a_form2):\n",
    "    matcher = SequenceMatcher(None, a_form1, a_form2)\n",
    "    split_form1 = []\n",
    "    split_form2 = []\n",
    "    \n",
    "    # get_opcodes()の出力を確認\n",
    "    opcodes = matcher.get_opcodes()\n",
    "    for tag, i1, i2, j1, j2 in opcodes:\n",
    "        if tag == 'equal':\n",
    "            split_form1.append(a_form1[i1:i2])\n",
    "            split_form2.append(a_form2[j1:j2])\n",
    "        elif tag == 'replace':\n",
    "            split_form1.append(a_form1[i1:i2])\n",
    "            split_form2.append(a_form2[j1:j2])\n",
    "        elif tag == 'delete':\n",
    "            split_form1.append(a_form1[i1:i2])\n",
    "            split_form2.append('')\n",
    "        elif tag == 'insert':\n",
    "            split_form1.append('')\n",
    "            split_form2.append(a_form2[j1:j2])\n",
    "    # 空の部分集合を削除        \n",
    "    split_form1 = [part for part in split_form1 if part]\n",
    "    split_form2 = [part for part in split_form2 if part]\n",
    "\n",
    "    return split_form1, split_form2\n",
    "\n",
    "def split_form_process(transformed_form_pairs):\n",
    "    split_form_pairs = []\n",
    "    for a_form1, a_form2 in transformed_form_pairs:\n",
    "        a_split_form_result = split_form_sim_diff_ability(a_form1, a_form2)\n",
    "        split_form_pairs.append(a_split_form_result)\n",
    "    return split_form_pairs\n",
    "\n",
    "def compare_forms_by_index_ability(a_form1_as_list, a_form2_as_list): \n",
    "    \n",
    "    # 2つのリスト長が異なる場合,'2'を返す\n",
    "    if len(a_form1_as_list) != len(a_form2_as_list):\n",
    "        return \"2\"\n",
    "    \n",
    "    comparison_form_result_by_index = []\n",
    "    \n",
    "    # リスト長が同じなら，共通部分＝0, 差異部分＝1\n",
    "    for index in range(len(a_form1_as_list)):\n",
    "        if a_form1_as_list[index] == a_form2_as_list[index]:\n",
    "            comparison_form_result_by_index.append('0')\n",
    "        else:\n",
    "            comparison_form_result_by_index.append('1')\n",
    "            \n",
    "    return ''.join(comparison_form_result_by_index)\n",
    "\n",
    "def compare_forms_by_index_process(split_form_pairs):\n",
    "    compare_form_pair_results = []\n",
    "    for a_form1_as_list, a_form2_as_list in split_form_pairs:\n",
    "        a_compare_form_pair_result = compare_forms_by_index_ability(a_form1_as_list, a_form2_as_list)\n",
    "        compare_form_pair_results.append(a_compare_form_pair_result)\n",
    "    return compare_form_pair_results\n",
    "\n",
    "def get_filtered_indices(compare_form_pair_results):\n",
    "    filtered_indices_set = []\n",
    "    for index, element in enumerate(compare_form_pair_results):\n",
    "        if (\n",
    "            2 <= len(element) <= 3 and\n",
    "            element.count('1') < 2 and\n",
    "            '00' not in element and\n",
    "            '11' not in element and\n",
    "            element != '2'\n",
    "        ):\n",
    "            filtered_indices_set.append(index)\n",
    "    return filtered_indices_set\n",
    "\n",
    "def final_transformed_set_form(can_chunk_semantic_form_pairs, rule_set, filtered_indices_set):\n",
    "    rule_dict = {}\n",
    "    for a_rule in rule_set:\n",
    "        key, value = a_rule.split(\" -> \")\n",
    "        rule_dict[key] = value\n",
    "\n",
    "    all_transformed_form_pairs = []\n",
    "    for index in filtered_indices_set:\n",
    "        transformed_form_pairs = []\n",
    "        selected_pair = can_chunk_semantic_form_pairs[index]\n",
    "\n",
    "        left, right, _ = selected_pair\n",
    "        left_form = f\"{left[0]}/{left[1]}({left[2]},{left[3]}){left[4]}\"\n",
    "        right_form = f\"{right[0]}/{right[1]}({right[2]},{right[3]}){right[4]}\"\n",
    "\n",
    "        left_transformed_form = rule_dict.get(left_form, \"\")\n",
    "        right_transformed_form = rule_dict.get(right_form, \"\")\n",
    "\n",
    "        if left_transformed_form:\n",
    "            transformed_form_pairs.append(f\"{left_form} -> {left_transformed_form}\")\n",
    "        if right_transformed_form:\n",
    "            transformed_form_pairs.append(f\"{right_form} -> {right_transformed_form}\")\n",
    "\n",
    "        all_transformed_form_pairs.append(transformed_form_pairs)\n",
    "\n",
    "    return all_transformed_form_pairs\n",
    "\n",
    "\"\"\"\n",
    "transformed_form_pairs = transformed_set_form(can_chunk_semantic_form_pairs, rule_set)\n",
    "split_form_pairs = split_form_process(transformed_form_pairs)\n",
    "form_pair_results = compare_forms_by_index_process(split_form_pairs)\n",
    "result_indices = get_filtered_indices(form_pair_results)\n",
    "can_chunk_rule_set = final_transformed_set_form(can_chunk_semantic_form_pairs, rule_set, result_indices)\n",
    "\n",
    "# print(can_chunk_rule_set)\n",
    "\"\"\"\n",
    "\n",
    "# 形式と意味に分割する能力\n",
    "def parse_rule(an_element_of_a_can_chunk_rule):\n",
    "    parts = an_element_of_a_can_chunk_rule.split('->')\n",
    "    semantic_structure = parts[0].strip()\n",
    "    form = parts[1].strip()\n",
    "    return semantic_structure, form\n",
    "\n",
    "# can_chunk_rule_setを， 意味表現だけのペアのリストに変換\n",
    "def transform_only_sem_chunk_pair(can_chunk_rule_set):\n",
    "    transform_only_sem_chunk_pair_sets = []\n",
    "\n",
    "    for a_can_chunk_rule in can_chunk_rule_set: # setから1つづつ呼び出す\n",
    "        transformed_a_can_chunk_rule_pair = []\n",
    "        for an_element_of_a_can_chunk_rule in a_can_chunk_rule:\n",
    "            semantic_structure, _ = parse_rule(an_element_of_a_can_chunk_rule) # リスト要素のペアをバラす\n",
    "            transformed_a_can_chunk_rule_pair.append(semantic_structure)\n",
    "        transform_only_sem_chunk_pair_sets.append(transformed_a_can_chunk_rule_pair)\n",
    "    \n",
    "    return transform_only_sem_chunk_pair_sets\n",
    "def split_semantics_ability(semantic_elements):\n",
    "    # 意味表現を単語単位で分割\n",
    "    return re.findall(r'_[a-zA-Z0-9]+|\\(\\w+\\)|[A-Z]+|/[0-9]', semantic_elements)\n",
    "\n",
    "def split_sem_pairs_for_chunk(transform_only_sem_chunk_pair_sets):\n",
    "    split_sem_pairs_for_chunk = []\n",
    "    for pair in transform_only_sem_chunk_pair_sets:\n",
    "        split_pair = []\n",
    "        for semantic_element in pair:\n",
    "            split_element = split_semantics_ability(semantic_element)\n",
    "            split_pair.append(split_element)\n",
    "        split_sem_pairs_for_chunk.append(split_pair)\n",
    "    return split_sem_pairs_for_chunk\n",
    "\n",
    "\n",
    "\n",
    "# 意味の差異部分のindexを検知する能力\n",
    "def detect_index_sem_difference_ability(split_sem1, split_sem2):\n",
    "    differing_indices = []\n",
    "    # split_semのlengthが同じであるか確認\n",
    "    for i in range(len(split_sem1)): \n",
    "        if split_sem1[i] != split_sem2[i]:\n",
    "            differing_indices.append(i)\n",
    "    return differing_indices\n",
    "\n",
    "# 意味の差異部分のindexを所得する\n",
    "def detect_index_sem_difference_process(split_sem_pairs_for_chunk):\n",
    "    index_sem_difference_set = []\n",
    "    for pair in split_sem_pairs_for_chunk:\n",
    "        # 修正点: pair[0] と pair[1] はリストなので、そのまま使用\n",
    "        differing_indices = detect_index_sem_difference_ability(pair[0], pair[1])\n",
    "        index_sem_difference_set.append(differing_indices)\n",
    "    return index_sem_difference_set\n",
    "\n",
    "\n",
    "# 形式と意味に分割する能力\n",
    "# 上と同じため省略\n",
    "\n",
    "# can_chunk_rule_setを， 形式表現だけのペアのリストに変換\n",
    "def transform_only_form_chunk_pair(can_chunk_rule_set):\n",
    "    transform_only_form_chunk_pair_sets = []\n",
    "\n",
    "    for a_can_chunk_rule in can_chunk_rule_set: # setから1つづつ呼び出す\n",
    "        transformed_a_can_chunk_rule_pair = []\n",
    "        for an_element_of_a_can_chunk_rule in a_can_chunk_rule:\n",
    "            _, form = parse_rule(an_element_of_a_can_chunk_rule) # リスト要素のペアをバラし，形式表現だけ抜く\n",
    "            transformed_a_can_chunk_rule_pair.append(form)\n",
    "        transform_only_form_chunk_pair_sets.append(transformed_a_can_chunk_rule_pair)\n",
    "    \n",
    "    return transform_only_form_chunk_pair_sets\n",
    "\n",
    "def split_form_sim_diff_ability(a_form1, a_form2):\n",
    "    matcher = SequenceMatcher(None, a_form1, a_form2)\n",
    "    split_form1 = []\n",
    "    split_form2 = []\n",
    "    \n",
    "    # get_opcodes()の出力を確認\n",
    "    opcodes = matcher.get_opcodes()\n",
    "    for tag, i1, i2, j1, j2 in opcodes:\n",
    "        if tag == 'equal':\n",
    "            split_form1.append(a_form1[i1:i2])\n",
    "            split_form2.append(a_form2[j1:j2])\n",
    "        elif tag == 'replace':\n",
    "            split_form1.append(a_form1[i1:i2])\n",
    "            split_form2.append(a_form2[j1:j2])\n",
    "        elif tag == 'delete':\n",
    "            split_form1.append(a_form1[i1:i2])\n",
    "            split_form2.append('')\n",
    "        elif tag == 'insert':\n",
    "            split_form1.append('')\n",
    "            split_form2.append(a_form2[j1:j2])\n",
    "    # 空の部分集合を削除        \n",
    "    split_form1 = [part for part in split_form1 if part]\n",
    "    split_form2 = [part for part in split_form2 if part]\n",
    "\n",
    "    return split_form1, split_form2\n",
    "\n",
    "def split_form_process(transform_only_form_chunk_pair_sets):\n",
    "    split_form_pairs_for_chunk = []\n",
    "    for a_form1, a_form2 in transform_only_form_chunk_pair_sets:\n",
    "        a_split_form_result = split_form_sim_diff_ability(a_form1, a_form2)\n",
    "        split_form_pairs_for_chunk.append(a_split_form_result)\n",
    "    return split_form_pairs_for_chunk\n",
    "\n",
    "\n",
    "\n",
    "def detect_index_form_difference_ability(a_form1_as_list, a_form2_as_list): \n",
    "    # 2つのリストの長さが異なる場合、\"長さが異なります\"を返す\n",
    "    if len(a_form1_as_list) != len(a_form2_as_list):\n",
    "        return \"長さが異なります\"\n",
    "    \n",
    "    differing_indices = []\n",
    "    \n",
    "    # リストの長さが同じ場合、差異部分のインデックスをリストに格納\n",
    "    for index in range(len(a_form1_as_list)):\n",
    "        if a_form1_as_list[index] != a_form2_as_list[index]:\n",
    "            differing_indices.append(index)\n",
    "    \n",
    "    return differing_indices\n",
    "\n",
    "\n",
    "\n",
    "def detect_index_form_difference_process(split_form_pairs_for_chunk):\n",
    "\n",
    "    index_form_difference_sets = []\n",
    "    for a_form1_as_list, a_form2_as_list in split_form_pairs_for_chunk:\n",
    "        differing_indices = detect_index_form_difference_ability(a_form1_as_list, a_form2_as_list)\n",
    "        index_form_difference_sets.append(differing_indices)\n",
    "    \n",
    "    return index_form_difference_sets\n",
    "\n",
    "def generate_random_label(used_labels):\n",
    "    excluded_labels = {'S', 'X', 'Y', 'P'}\n",
    "    available_labels = list(set(string.ascii_uppercase) - used_labels - excluded_labels)\n",
    "    label = random.choice(available_labels)\n",
    "    used_labels.add(label)\n",
    "    return label\n",
    "\n",
    "def apply_existing_labels_for_type2_chunk(used_labels, split_form_pairs_for_chunk):\n",
    "    for split_form_pair_for_chunk in split_form_pairs_for_chunk:\n",
    "        for split_form_for_chunk in split_form_pair_for_chunk:\n",
    "            for an_element_of_split_form_for_chunk in split_form_for_chunk:\n",
    "                if '/' in an_element_of_split_form_for_chunk:\n",
    "                    label, _ = an_element_of_split_form_for_chunk.split('/')\n",
    "                    used_labels.add(label)\n",
    "                    \n",
    "def chunk_completed_to_generate_scheme_rules_and_word_rules(\n",
    "    split_sem_pairs_for_chunk,\n",
    "    split_form_pairs_for_chunk,\n",
    "    index_sem_difference_sets,\n",
    "    index_form_difference_sets\n",
    "):\n",
    "    chunk_completed_to_generate_scheme_rules_and_word_rules_pairs = []\n",
    "    used_labels = set()\n",
    "    \n",
    "    # 既存のラベルをused_labelsに追加\n",
    "    apply_existing_labels_for_type2_chunk(used_labels, split_form_pairs_for_chunk)\n",
    "\n",
    "    # インデックスと変項の対応関係を辞書で定義\n",
    "    index_to_var = {1: '_p', 2: '_x', 3: '_y'}\n",
    "\n",
    "    for sem_pair, form_pair, sem_diff, form_diff in zip(\n",
    "        split_sem_pairs_for_chunk,\n",
    "        split_form_pairs_for_chunk,\n",
    "        index_sem_difference_sets,\n",
    "        index_form_difference_sets\n",
    "    ):\n",
    "        # 形式ルールの方に既にラベルが存在する場合は、そのラベルを採用\n",
    "        existing_label = None\n",
    "        for form_elements in form_pair:  # 修正：両方のペアをチェック\n",
    "            for an_element_of_split_form_for_chunk in form_elements:\n",
    "                if '/' in an_element_of_split_form_for_chunk:\n",
    "                    label, _ = an_element_of_split_form_for_chunk.split('/')\n",
    "                    existing_label = label\n",
    "                    break\n",
    "            if existing_label is not None:\n",
    "                break\n",
    "        \n",
    "        if existing_label is not None:\n",
    "            label = existing_label\n",
    "        else:\n",
    "            label = generate_random_label(used_labels)\n",
    "\n",
    "        # インデックスに基づいて変項を決定\n",
    "        index = sem_diff[0]\n",
    "        var = index_to_var.get(index, '_x')  # デフォルトは '_x'\n",
    "\n",
    "        # スキーマルールの意味側\n",
    "        sem_of_scheme_rule = sem_pair[0][:]\n",
    "        sem_of_scheme_rule[index] = var\n",
    "        # スキーマルールの形式側\n",
    "        form_of_scheme_rule = form_pair[0][:]\n",
    "        form_of_scheme_rule[form_diff[0]] = f'{label}/{var[1]}'\n",
    "        # スキーマルールの完成：意味側＋形式側\n",
    "        scheme_rule = [sem_of_scheme_rule, form_of_scheme_rule]\n",
    "        \n",
    "        # 単語ルールの意味側\n",
    "        sem_of_word_rule_1 = [f'{label}', sem_pair[0][index]]\n",
    "        sem_of_word_rule_2 = [f'{label}', sem_pair[1][index]]\n",
    "        # 単語ルールの形式側\n",
    "        form_of_word_rule_1 = [form_pair[0][form_diff[0]]]\n",
    "        form_of_word_rule_2 = [form_pair[1][form_diff[0]]]\n",
    "        # 単語ルールの完成：意味側＋形式側\n",
    "        word_rule_1 = [sem_of_word_rule_1, form_of_word_rule_1]\n",
    "        word_rule_2 = [sem_of_word_rule_2, form_of_word_rule_2]\n",
    "        \n",
    "        # _p, _x, _yが含まれる単語ルールの削除\n",
    "        word_rules = []\n",
    "        unwanted_vars = ['_p', '_x', '_y']\n",
    "        for word_rule in [word_rule_1, word_rule_2]:\n",
    "            if not any(var in word_rule[0] for var in unwanted_vars):\n",
    "                word_rules.append(word_rule)\n",
    "\n",
    "        # ルールの保存\n",
    "        chunk_completed_to_generate_scheme_rules_and_word_rules_pairs.append((scheme_rule, *word_rules))\n",
    "        \n",
    "    return chunk_completed_to_generate_scheme_rules_and_word_rules_pairs\n",
    "\n",
    "\n",
    "# ----------------   ここから実行    -------------------------\n",
    "\n",
    "# 意味側\n",
    "semantic_set = set_semantics(rule_set)\n",
    "split_semantic_elements = split_semantics_process(semantic_set)\n",
    "can_chunk_semantic_form_pairs = detect_sem_pairs_with_only_one_difference(split_semantic_elements)\n",
    "# print(can_chunk_semantic_form_pairs)\n",
    "\n",
    "# 形式側\n",
    "transformed_form_pairs = transformed_set_form(can_chunk_semantic_form_pairs, rule_set)\n",
    "split_form_pairs = split_form_process(transformed_form_pairs)\n",
    "form_pair_results = compare_forms_by_index_process(split_form_pairs)\n",
    "result_indices = get_filtered_indices(form_pair_results)\n",
    "can_chunk_rule_set = final_transformed_set_form(can_chunk_semantic_form_pairs, rule_set, result_indices)\n",
    "# print(can_chunk_rule_set)\n",
    "\n",
    "# 各ステップの処理を実行\n",
    "transform_only_sem_chunk_pairs = transform_only_sem_chunk_pair(can_chunk_rule_set)\n",
    "split_sem_pairs_for_chunk = split_sem_pairs_for_chunk(transform_only_sem_chunk_pairs)\n",
    "index_sem_difference_sets = detect_index_sem_difference_process(split_sem_pairs_for_chunk)\n",
    "\n",
    "transform_only_form_chunk_pairs = transform_only_form_chunk_pair(can_chunk_rule_set)\n",
    "split_form_pairs_for_chunk = split_form_process(transform_only_form_chunk_pairs)\n",
    "index_form_difference_sets = detect_index_form_difference_process(split_form_pairs_for_chunk)\n",
    "\n",
    "chunk_completed_to_generate_scheme_rules_and_word_rules_pairs = chunk_completed_to_generate_scheme_rules_and_word_rules(\n",
    "    split_sem_pairs_for_chunk,\n",
    "    split_form_pairs_for_chunk,\n",
    "    index_sem_difference_sets,\n",
    "    index_form_difference_sets\n",
    ")\n",
    "\n",
    "# ルールセット全体をコピーして保持\n",
    "remaining_rules = rule_set[:]\n",
    "\n",
    "# can_chunk_rule_set に含まれるルールを used_rules に追加\n",
    "used_rules = []\n",
    "for rule_pair in can_chunk_rule_set:\n",
    "    for rule in rule_pair:\n",
    "        # 元の rule_set にあるかを確認し、あれば used_rules に追加\n",
    "        for original_rule in rule_set:\n",
    "            if rule in original_rule:\n",
    "                used_rules.append(original_rule)\n",
    "\n",
    "# 未使用のルールを特定\n",
    "unapplied_rules = [rule for rule in remaining_rules if rule not in used_rules]\n",
    "\n",
    "chunked_rules = []\n",
    "\n",
    "# chunk_completed_to_generate_scheme_rules_and_word_rules_pairs のルールを整形して chunked_rules に追加\n",
    "for scheme_rule, *word_rules in chunk_completed_to_generate_scheme_rules_and_word_rules_pairs:\n",
    "    # スキーマルールの整形\n",
    "    sem_scheme_rule = scheme_rule[0]\n",
    "    \n",
    "    # 2番目と3番目の要素をまとめて括弧で囲む\n",
    "    if len(sem_scheme_rule) >= 4:  # 安全のために要素数が十分にあるか確認\n",
    "        combined_element = f\"({sem_scheme_rule[2]},{sem_scheme_rule[3]})\"\n",
    "        sem_scheme_rule = sem_scheme_rule[:2] + [combined_element] + sem_scheme_rule[4:]\n",
    "\n",
    "    # スキーマルールを文字列に結合\n",
    "    sem_scheme_rule = f\"{sem_scheme_rule[0]}/\" + \"\".join(sem_scheme_rule[1:])\n",
    "    form_scheme_rule = \"\".join(scheme_rule[1])\n",
    "\n",
    "    chunked_rules.append(f\"{sem_scheme_rule} -> {form_scheme_rule}\")\n",
    "    \n",
    "    # 単語ルールの整形\n",
    "    for word_rule in word_rules:\n",
    "        sem_word_rule = \"/\".join(word_rule[0])\n",
    "        form_word_rule = \"\".join(word_rule[1])\n",
    "        chunked_rules.append(f\"{sem_word_rule} -> {form_word_rule}\")\n",
    "\n",
    "# unapplied_rules を chunked_rules に追加\n",
    "chunked_rules.extend(unapplied_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db739e65-83dc-4fba-b30c-60a191bf3e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunked_rules' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunked_rules)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chunked_rules' is not defined"
     ]
    }
   ],
   "source": [
    "print(chunked_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
