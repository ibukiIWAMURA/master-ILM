{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a55eaeb-b3f8-47e4-9b62-ab1a03df68eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = ['S/_know(_david,_y)/0 -> wB/yetk', 'B/_eve -> bo', 'S/_meet(_x,_alice)/0 -> bE/x', 'E/_eve -> khcj', 'E/_carol -> rlatv', 'S/_like(_bob,_y)/0 -> foJ/y', 'J/_eve -> vhddl', 'J/_alice -> cnwbjk', 'S/_know(_x,_bob)/0 -> cB/x', 'B/_alice -> lmt', 'S/_like(_carol,_bob)/0 -> ufdr', 'S/_kick(_david,_bob)/0 -> fngkwul', 'S/_know(_david,_alice)/0 -> tmhrj', 'S/_meet(_david,_bob)/0 -> fndbak', 'S/_like(_david,_carol)/0 -> dldtkeb', 'S/_admire(_bob,_alice)/0 -> anhz', 'S/_admire(_eve,_david)/0 -> zqvtwkkm', 'S/_admire(_david,_carol)/0 -> fwftincab', 'S/_know(_bob,_eve)/0 -> rvgll', 'S/_admire(_bob,_david)/0 -> wcjaqoatd', 'S/_know(_carol,_bob)/0 -> iduful', 'S/_admire(_carol,_alice)/0 -> okoum', 'S/_kick(_bob,_david)/0 -> uznubut', 'S/_kick(_bob,_eve)/0 -> blcnl', 'S/_kick(_carol,_alice)/0 -> beeactrjb', 'S/_admire(_alice,_eve)/0 -> obhimme', 'S/_kick(_alice,_carol)/0 -> koiivdgjt', 'S/_meet(_bob,_david)/0 -> gec', 'S/_like(_bob,_carol)/0 -> lrohma', 'S/_kick(_eve,_carol)/0 -> rwmtjkuli', 'S/_meet(_alice,_carol)/0 -> dmtqim', 'S/_admire(_alice,_david)/0 -> dqd', 'S/_like(_alice,_eve)/0 -> jmzwakni', 'S/_know(_david,_eve)/0 -> wboetk', 'S/_meet(_eve,_carol)/0 -> vwed', 'S/_meet(_eve,_bob)/0 -> udv', 'S/_admire(_carol,_david)/0 -> vmuhvwq', 'S/_kick(_david,_carol)/0 -> cdhgk', 'S/_know(_carol,_david)/0 -> umlq', 'S/_meet(_eve,_david)/0 -> ieircfbg', 'S/_know(_david,_bob)/0 -> autlk', 'S/_like(_eve,_david)/0 -> enhnr', 'S/_admire(_david,_bob)/0 -> hciq', 'S/_kick(_bob,_carol)/0 -> oltjke', 'S/_know(_carol,_eve)/0 -> eqnaczd', 'S/_like(_eve,_alice)/0 -> nccf', 'S/_admire(_carol,_bob)/0 -> udoa', 'S/_kick(_alice,_bob)/0 -> uuwa', 'S/_kick(_eve,_alice)/0 -> oacmc', 'S/_kick(_eve,_david)/0 -> jfaizcku', 'S/_know(_alice,_eve)/0 -> nhl', 'S/_like(_carol,_alice)/0 -> amuuzh', 'S/_admire(_eve,_alice)/0 -> wfcnu', 'S/_meet(_david,_eve)/0 -> bwnktztq']\n",
    "only_sem_express_set_for_production = ['S/_know(_alice,_eve)/0', 'S/_meet(_alice,_david)/0', 'S/_kick(_alice,_eve)/0', 'S/_know(_carol,_eve)/0', 'S/_know(_bob,_alice)/0', 'S/_meet(_alice,_eve)/0', 'S/_know(_bob,_david)/0', 'S/_admire(_eve,_david)/0', 'S/_know(_bob,_carol)/0', 'S/_kick(_alice,_bob)/0', 'S/_kick(_carol,_david)/0', 'S/_know(_carol,_alice)/0', 'S/_meet(_carol,_david)/0', 'S/_kick(_eve,_alice)/0', 'S/_kick(_eve,_david)/0', 'S/_meet(_bob,_alice)/0', 'S/_like(_carol,_eve)/0', 'S/_admire(_alice,_carol)/0', 'S/_know(_eve,_alice)/0', 'S/_kick(_alice,_carol)/0', 'S/_meet(_carol,_bob)/0', 'S/_like(_carol,_alice)/0', 'S/_like(_carol,_david)/0', 'S/_like(_alice,_eve)/0', 'S/_kick(_bob,_david)/0', 'S/_like(_bob,_david)/0', 'S/_kick(_carol,_alice)/0', 'S/_admire(_bob,_alice)/0', 'S/_like(_bob,_alice)/0', 'S/_admire(_carol,_bob)/0', 'S/_meet(_eve,_david)/0', 'S/_like(_david,_bob)/0', 'S/_like(_eve,_alice)/0', 'S/_admire(_david,_carol)/0', 'S/_meet(_carol,_alice)/0', 'S/_like(_alice,_david)/0', 'S/_kick(_carol,_eve)/0', 'S/_know(_eve,_bob)/0', 'S/_admire(_bob,_david)/0', 'S/_kick(_bob,_alice)/0', 'S/_admire(_david,_bob)/0', 'S/_know(_bob,_eve)/0', 'S/_meet(_bob,_eve)/0', 'S/_kick(_david,_eve)/0', 'S/_meet(_bob,_david)/0', 'S/_know(_alice,_david)/0', 'S/_like(_bob,_eve)/0', 'S/_like(_david,_eve)/0', 'S/_admire(_carol,_alice)/0', 'S/_admire(_david,_alice)/0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0333ca67-b1a4-4ac1-83cf-23331c409e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = [\n",
    "    \"S/_believe(_ibuki,_quolia)/0 -> affordance\",\n",
    "    \"S/_help(_alice,_blackawa)/0 -> communication\",\n",
    "    \"S/_follow(_McKirby,_Kalin)/0 -> autopoiesis\",\n",
    "    \"S/_love(_x,_koko)/0 -> jazzT/x\",\n",
    "    \"S/_love(_x,_koko)/0 -> T/xhiphop\",\n",
    "    \"S/_love(_ibuki,_y)/0 -> E/yhiphop\",\n",
    "    \"S/_p(_hash,_y)/0 -> D/ybakabakkaW/p\",\n",
    "    \"S/_p(_x,_y)/0 -> W/pT/yD/x\",\n",
    "    \"W/_kill -> iwrhtb\",\n",
    "    \"D/_ibuki -> pow\",\n",
    "    \"T/_blackawa -> ljk\"\n",
    "]\n",
    "\n",
    "only_sem_express_set_for_production =[\n",
    "    \"S/_believe(_ibuki,_quolia)/0\", # 全体論的 ：　\" affordance　 \"\n",
    "    \"S/_help(_alice,_blackawa)/0\", # 全体論的 ：　\" communication　 \"\n",
    "    \"S/_believe(_blackawa,_quolia)/0\",# 全部invention\n",
    "    \"S/_help(_ibuki,_quolia)/0\", #  全部invention\n",
    "    \"S/_love(_ibuki,_koko)/0\", # jazz-[invention] or [invention]-hiphop\n",
    "    \"S/_love(_blackawa,_koko)/0\", # jazz-ljk  or  ljk-hiphop\n",
    "    \"S/_kill(_hash,_ibuki)/0\", # pow-bakabakka-iwrhtb\n",
    "    \"S/_kill(_hash,_koko)/0\", # [invention]-bakkabakka-iwrhtb\n",
    "    \"S/_kill(_ibuki,_blackawa)/0\", # iwrhtb-ljk-pow\n",
    "    \"S/_kill(_ibuki,_koko)/0\", # iwrhtb-[invention]-pow\n",
    "    \"S/_help(_ibuki,_koko)/0\", # 全部invention\n",
    "    \"S/_help(_sakana,_koko)/0\", # 全部invention\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d4b8f95-9ffc-4ba7-be40-5e8221e7ba29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = ['S/_like(_bob,_y)/0 -> foF/y', 'F/_alice -> cnwbjk', 'F/_eve -> vhddl', 'S/_meet(_x,_eve)/0 -> kT/xf', 'T/_alice -> djec', 'T/_bob -> miwm', 'S/_p(_carol,_alice)/0 -> bU/p', 'U/_know -> bdwjougu', 'U/_meet -> rlatv', 'S/_like(_david,_bob)/0 -> kbn', 'S/_admire(_bob,_alice)/0 -> anhz', 'S/_admire(_eve,_bob)/0 -> wdti', 'S/_admire(_david,_bob)/0 -> hciq', 'S/_like(_bob,_david)/0 -> wtfbm', 'S/_like(_carol,_bob)/0 -> ufdr', 'S/_admire(_carol,_david)/0 -> vmuhvwq', 'S/_admire(_bob,_carol)/0 -> iiltaawnr', 'S/_kick(_bob,_eve)/0 -> blcnl', 'S/_like(_alice,_bob)/0 -> vhgbog', 'S/_meet(_eve,_david)/0 -> ieircfbg', 'S/_kick(_alice,_bob)/0 -> uuwa', 'S/_know(_alice,_carol)/0 -> knjtvg', 'S/_meet(_carol,_david)/0 -> qbk', 'S/_like(_alice,_eve)/0 -> jmzwakni', 'S/_meet(_david,_carol)/0 -> qvgn', 'S/_kick(_alice,_carol)/0 -> koiivdgjt', 'S/_admire(_carol,_eve)/0 -> ubrvoqn', 'S/_admire(_bob,_david)/0 -> wcjaqoatd', 'S/_kick(_eve,_alice)/0 -> oacmc', 'S/_meet(_carol,_eve)/0 -> wcwfbndu', 'S/_like(_bob,_carol)/0 -> lrohma', 'S/_kick(_carol,_alice)/0 -> beeactrjb', 'S/_kick(_eve,_bob)/0 -> jcfmgejdf', 'S/_know(_bob,_alice)/0 -> cvjhvdlfc', 'S/_kick(_bob,_carol)/0 -> oltjke', 'S/_kick(_bob,_david)/0 -> uznubut', 'S/_admire(_carol,_alice)/0 -> okoum', 'S/_meet(_alice,_bob)/0 -> govdb', 'S/_know(_david,_carol)/0 -> ggafelht', 'S/_kick(_eve,_carol)/0 -> rwmtjkuli', 'S/_meet(_bob,_david)/0 -> gec', 'S/_like(_alice,_david)/0 -> rakrvu', 'S/_know(_alice,_david)/0 -> rumiuaeoo', 'S/_admire(_david,_alice)/0 -> ojk', 'S/_admire(_bob,_eve)/0 -> mqw', 'S/_kick(_david,_bob)/0 -> fngkwul', 'S/_know(_carol,_david)/0 -> umlq', 'S/_like(_david,_eve)/0 -> qbbctbuiz', 'S/_know(_eve,_carol)/0 -> rldvm', 'S/_kick(_alice,_eve)/0 -> ecql', 'S/_know(_david,_eve)/0 -> wboetk', 'S/_like(_alice,_carol)/0 -> nakbnlo', 'S/_know(_carol,_eve)/0 -> eqnaczd']\n",
    "only_sem_express_set_for_production = ['S/_know(_alice,_carol)/0', 'S/_kick(_eve,_bob)/0', 'S/_like(_eve,_alice)/0', 'S/_kick(_david,_alice)/0', 'S/_know(_bob,_alice)/0', 'S/_meet(_bob,_carol)/0', 'S/_like(_carol,_alice)/0', 'S/_admire(_david,_carol)/0', 'S/_like(_bob,_alice)/0', 'S/_know(_eve,_carol)/0', 'S/_like(_bob,_carol)/0', 'S/_like(_alice,_david)/0', 'S/_like(_carol,_eve)/0', 'S/_like(_alice,_bob)/0', 'S/_admire(_bob,_alice)/0', 'S/_know(_bob,_carol)/0', 'S/_admire(_carol,_david)/0', 'S/_meet(_bob,_alice)/0', 'S/_meet(_eve,_alice)/0', 'S/_admire(_david,_bob)/0', 'S/_meet(_eve,_carol)/0', 'S/_meet(_carol,_david)/0', 'S/_admire(_david,_alice)/0', 'S/_kick(_david,_bob)/0', 'S/_kick(_bob,_carol)/0', 'S/_admire(_carol,_alice)/0', 'S/_like(_carol,_david)/0', 'S/_like(_bob,_eve)/0', 'S/_like(_bob,_david)/0', 'S/_admire(_eve,_david)/0', 'S/_admire(_eve,_alice)/0', 'S/_know(_alice,_eve)/0', 'S/_meet(_david,_eve)/0', 'S/_like(_eve,_carol)/0', 'S/_kick(_alice,_eve)/0', 'S/_meet(_alice,_bob)/0', 'S/_admire(_alice,_carol)/0', 'S/_know(_carol,_bob)/0', 'S/_like(_eve,_david)/0', 'S/_kick(_carol,_alice)/0', 'S/_admire(_carol,_eve)/0', 'S/_meet(_alice,_carol)/0', 'S/_admire(_alice,_david)/0', 'S/_know(_david,_eve)/0', 'S/_meet(_bob,_david)/0', 'S/_meet(_david,_alice)/0', 'S/_admire(_alice,_bob)/0', 'S/_meet(_alice,_david)/0', 'S/_know(_carol,_eve)/0', 'S/_know(_alice,_bob)/0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39f30e2-1ef8-420d-8684-076e101a3dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = [\n",
    "    \"S/_believe(_ibuki, _quolia)/0 -> affordance\",\n",
    "    \"S/_help(_alice, _blackawa)/0 -> communication\",\n",
    "    \"S/_follow(_McKirby, _Kalin)/0 -> autopoiesis\",\n",
    "    \"S/_love(_x, _koko)/0 -> jazzΠ/x\",\n",
    "    \"S/_love(_x, _koko)/0 -> Π/xhiphop\",\n",
    "    \"S/_love(_ibuki, _y)/0 -> E/yhiphop\",\n",
    "    \"S/_p(_hash, _y)/0 -> D/ybakabakkaW/p\",\n",
    "    # \"S/_p(_x, _y)/0 -> W/pT/yD/x\",\n",
    "    \"W/_kill -> iwrhtb\",\n",
    "    \"D/_ibuki -> pow\",\n",
    "    \"Π/_blackawa -> ljk\"\n",
    "]\n",
    "\n",
    "only_sem_express_set_for_production =[\n",
    "    \"S/_believe(_ibuki, _quolia)/0\", # 全体論的 ：　\" affordance　 \"\n",
    "    \"S/_help(_alice, _blackawa)/0\", # 全体論的 ：　\" communication　 \"\n",
    "    \"S/_believe(_blackawa, _quolia)/0\",# 全部invention\n",
    "    \"S/_help(_ibuki, _quolia)/0\", #  全部invention\n",
    "    \"S/_love(_ibuki, _koko)/0\", # jazz-[invention] or [invention]-hiphop\n",
    "    \"S/_love(_blackawa, _koko)/0\", # jazz-ljk  or  ljk-hiphop\n",
    "    \"S/_kill(_hash, _ibuki)/0\", # pow-bakabakka-iwrhtb\n",
    "    \"S/_kill(_hash, _koko)/0\", # [invention]-bakkabakka-iwrhtb\n",
    "    \"S/_kill(_ibuki, _blackawa)/0\", # iwrhtb-ljk-pow\n",
    "    \"S/_kill(_ibuki, _koko)/0\", # iwrhtb-[invention]-pow\n",
    "    \"S/_help(_ibuki, _koko)/0\", # 全部invention\n",
    "    \"S/_help(_sakana, _koko)/0\", # 全部invention\n",
    "]\n",
    "# Π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c6550e-1419-413f-8714-8e65cc18f314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = ['S/_p(_carol,_y)/0->wzG/phlΑ/yafew', 'Α/_bob->b', 'S/_p(_david,_y)/0->nccD/pΑ/yamiq', 'S/_p(_david,_y)/0->nccB/pΑ/yamiq', 'S/_p(_david,_y)/0->nccM/pΑ/yamiq', 'S/_p(_david,_y)/0->nccW/pΑ/yamiq', 'S/_p(_david,_y)/0->nccK/pΑ/yamiq', 'S/_p(_david,_y)/0->nccΖ/pΑ/yamiq', 'S/_p(_x,_carol)/0->Α/xiH/pwf', 'S/_admire(_eve,_y)/0->dΑ/yqled', 'S/_admire(_x,_alice)/0->ufwhgΙ/xvfΑ/xM/y', 'S/_admire(_x,_eve)/0->ljjbiΝ/xej', 'Ν/_david->h', 'S/_meet(_x,_carol)/0->cwqoΝ/xdjv', 'S/_p(_carol,_y)/0->wzG/phlL/yfew', 'L/_bob->ba', 'S/_p(_david,_y)/0->nccD/pL/ymiq', 'S/_p(_david,_y)/0->nccB/pL/ymiq', 'S/_p(_david,_y)/0->nccM/pL/ymiq', 'S/_p(_david,_y)/0->nccW/pL/ymiq', 'S/_p(_david,_y)/0->nccK/pL/ymiq', 'S/_p(_david,_y)/0->nccΖ/pL/ymiq', 'S/_p(_david,_y)/0->nccM/pΔ/ymiq', 'M/_like->m', 'S/_p(_eve,_carol)/0->uM/pc', 'S/_p(_carol,_y)/0->wzG/phlO/yfew', 'O/_bob->ba', 'S/_p(_david,_y)/0->nccD/pO/ymiq', 'S/_p(_david,_y)/0->nccB/pO/ymiq', 'S/_p(_david,_y)/0->nccM/pO/ymiq', 'S/_p(_david,_y)/0->nccW/pO/ymiq', 'S/_p(_david,_y)/0->nccK/pO/ymiq', 'S/_p(_david,_y)/0->nccΖ/pO/ymiq', 'S/_p(_david,_y)/0->nccU/pL/ymiq', 'L/_alice->o', 'S/_know(_eve,_y)/0->gkfL/yzoe', 'H/_like->d', 'S/_p(_carol,_y)/0->wzH/phlO/yfew', 'S/_p(_x,_eve)/0->eU/pΙ/x', 'U/_meet->j', 'S/_p(_david,_carol)/0->cwqohdU/pv', 'S/_p(_carol,_y)/0->wzG/phlΔ/yfew', 'Δ/_bob->ba', 'S/_p(_david,_y)/0->nccD/pΔ/ymiq', 'S/_p(_david,_y)/0->nccB/pΔ/ymiq', 'S/_p(_david,_y)/0->nccW/pΔ/ymiq', 'S/_p(_david,_y)/0->nccK/pΔ/ymiq', 'S/_p(_david,_y)/0->nccΖ/pΔ/ymiq', 'S/_p(_x,_carol)/0->Α/xiG/pwf', 'G/_like->d', 'S/_p(_x,_carol)/0->Α/xiJ/pwf', 'J/_like->d', 'S/_p(_carol,_y)/0->wzJ/phlO/yfew', 'S/_p(_david,_y)/0->nccU/pΚ/ymiq', 'Κ/_alice->o', 'S/_know(_eve,_y)/0->gkfΚ/yzoe', 'S/_admire(_eve,_y)/0->ufΗ/xΗ/yzvfbM/y', 'Η/_carol->cg', 'S/_p(_carol,_y)/0->wzG/phlN/yfew', 'N/_bob->ba', 'S/_p(_david,_y)/0->nccD/pN/ymiq', 'S/_p(_david,_y)/0->nccB/pN/ymiq', 'S/_p(_david,_y)/0->nccM/pN/ymiq', 'S/_p(_david,_y)/0->nccW/pN/ymiq', 'S/_p(_david,_y)/0->nccK/pN/ymiq', 'S/_p(_david,_y)/0->nccΖ/pN/ymiq', 'Ζ/_like->m', 'S/_p(_eve,_carol)/0->uΖ/pc', 'S/_p(_x,_eve)/0->eΖ/pΙ/x', 'Ζ/_meet->j', 'S/_p(_david,_carol)/0->cwqohdΖ/pv', 'S/_p(_carol,_y)/0->wzG/phlA/yfew', 'A/_bob->ba', 'S/_p(_david,_y)/0->nccD/pA/ymiq', 'S/_p(_david,_y)/0->nccB/pA/ymiq', 'S/_p(_david,_y)/0->nccM/pA/ymiq', 'S/_p(_david,_y)/0->nccW/pA/ymiq', 'S/_p(_david,_y)/0->nccK/pA/ymiq', 'S/_p(_david,_y)/0->nccΖ/pA/ymiq', 'S/_p(_david,_y)/0->nccU/pΝ/ymiq', 'Ν/_alice->o', 'S/_know(_eve,_y)/0->gkfΝ/yzoe', 'S/_p(_david,_y)/0->nccU/pΔ/ymiq', 'U/_like->m', 'S/_p(_eve,_carol)/0->uU/pc', 'S/_p(_x,_eve)/0->eB/pΙ/x', 'B/_meet->j', 'S/_p(_david,_carol)/0->cwqohdB/pv', 'S/_p(_x,_carol)/0->Β/xiΓ/pwf', 'Γ/_kick->u', 'Δ/_alice->o', 'S/_know(_eve,_y)/0->gkfΔ/yzoe', 'Α/_david->k', 'S/_p(_x,_carol)/0->Α/xiΦ/pwf', 'S/_p(_x,_carol)/0->Α/xiΓ/pwf', 'K/_like->m', 'S/_p(_eve,_carol)/0->uK/pc', 'S/_p(_carol,_eve)/0->wznhD/pdfew', 'D/_know->l', 'S/_p(_x,_eve)/0->eW/pΙ/x', 'W/_meet->j', 'S/_p(_david,_carol)/0->cwqohdW/pv', 'Γ/_like->d', 'S/_p(_carol,_y)/0->wzΓ/phlO/yfew', 'S/_p(_carol,_eve)/0->wznhB/pdfew', 'B/_know->l', 'S/_p(_x,_carol)/0->Β/xiΦ/pwf', 'Φ/_kick->u', 'S/_admire(_x,_bob)/0->Η/xbqled', 'Η/_eve->d', 'S/_kick(_carol,_y)/0->tgrΗ/yozhzc', 'S/_know(_carol,_y)/0->wznhlΗ/yfew', 'S/_kick(_bob,_y)/0->vΗ/yt', 'S/_p(_david,_y)/0->kiJ/pwΙ/y', 'Ι/_carol->f', 'S/_p(_david,_y)/0->kiΦ/pwΙ/y', 'S/_like(_x,_y)/0->Α/xidwΙ/y', 'S/_p(_david,_y)/0->kiΓ/pwΙ/y', 'S/_p(_x,_bob)/0->wzG/phlbaΙ/xew', 'S/_like(_x,_y)/0->wzdhlO/yΙ/xew', 'S/_p(_bob,_y)/0->biH/pwΙ/y', 'S/_kick(_x,_y)/0->Β/xiuwΙ/y', 'S/_admire(_eve,_y)/0->uΙ/yΗ/xcgzvfbM/y', 'S/_know(_x,_eve)/0->wznhldΙ/xew', 'W/_kick->iui', 'J/_admire->ft', 'D/_kick->iui', 'K/_kick->iui', 'G/_meet->lui', 'Ι/_bob->ukn', 'S/_admire(_alice,_y)/0->lbkA/ynmdm', 'S/_admire(_alice,_y)/0->lbkΗ/ynmdm', 'S/_know(_bob,_y)/0->zΝ/yijufd', 'S/_know(_x,_carol)/0->ebΚ/xwcnd', 'S/_admire(_alice,_y)/0->lbkN/ynmdm', 'S/_kick(_bob,_y)/0->T/yf', 'T/_carol->biuw', 'T/_alice->hqc', 'S/_meet(_alice,_david)/0->lqmul', 'S/_admire(_david,_alice)/0->ufwΙ/xvfbM/y', 'S/_admire(_carol,_alice)/0->utlrbzci', 'S/_know(_carol,_alice)/0->etvi', 'S/_like(_carol,_david)/0->oggizzu', 'S/_like(_alice,_eve)/0->ggqtvwh']\n",
    "only_sem_express_set_for_production = ['S/_like(_alice,_bob)/0', 'S/_meet(_bob,_david)/0', 'S/_kick(_carol,_david)/0', 'S/_know(_bob,_carol)/0', 'S/_meet(_alice,_eve)/0', 'S/_like(_david,_alice)/0', 'S/_admire(_eve,_alice)/0', 'S/_admire(_david,_eve)/0', 'S/_know(_carol,_david)/0', 'S/_like(_alice,_eve)/0', 'S/_like(_carol,_david)/0', 'S/_kick(_eve,_alice)/0', 'S/_like(_carol,_bob)/0', 'S/_know(_carol,_alice)/0', 'S/_kick(_eve,_carol)/0', 'S/_admire(_alice,_carol)/0', 'S/_meet(_eve,_alice)/0', 'S/_know(_david,_bob)/0', 'S/_meet(_david,_alice)/0', 'S/_kick(_david,_bob)/0', 'S/_like(_bob,_david)/0', 'S/_meet(_alice,_bob)/0', 'S/_know(_alice,_carol)/0', 'S/_like(_alice,_carol)/0', 'S/_admire(_bob,_eve)/0', 'S/_admire(_eve,_carol)/0', 'S/_admire(_eve,_david)/0', 'S/_know(_david,_eve)/0', 'S/_know(_bob,_david)/0', 'S/_meet(_eve,_bob)/0', 'S/_kick(_bob,_alice)/0', 'S/_meet(_bob,_eve)/0', 'S/_kick(_david,_eve)/0', 'S/_like(_carol,_eve)/0', 'S/_meet(_carol,_bob)/0', 'S/_like(_eve,_carol)/0', 'S/_admire(_david,_carol)/0', 'S/_know(_bob,_alice)/0', 'S/_kick(_alice,_carol)/0', 'S/_know(_alice,_david)/0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb3a105f-ca1e-4408-b8aa-a5a690e7c609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rule_set = ['S/_admire(_x,_y)/0->ufΗ/xozvfbM/y']\n",
    "only_sem_express_set_for_production = ['S/_admire(_eve,_alice)/0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53fe4b90-fee3-4484-82f5-df4dfd989bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "from difflib import SequenceMatcher\n",
    "import string\n",
    "import random\n",
    "\n",
    "# chunkから借用\n",
    "def parse_rule(rule):\n",
    "    parts = rule.split('->')\n",
    "    semantic_structure = parts[0].strip()\n",
    "    form = parts[1].strip()\n",
    "    return semantic_structure, form\n",
    "\n",
    "def set_semantics(rule_set):\n",
    "    semantic_set = []\n",
    "    for a_rule in rule_set:\n",
    "        a_semantics = parse_rule(a_rule)[0]\n",
    "        semantic_set.append(a_semantics)\n",
    "    return semantic_set\n",
    "\n",
    "def clustering_rule_set(rule_set):\n",
    "    holistic_rule_set = []\n",
    "    generalization_rule_set_1 = []\n",
    "    generalization_rule_set_2 = []\n",
    "    generalization_rule_set_3 = []\n",
    "    word_rule_set = []\n",
    "\n",
    "    for rule in rule_set:\n",
    "        semantic_structure,_ = parse_rule(rule)\n",
    "\n",
    "        if not semantic_structure.startswith(\"S/\"):\n",
    "            word_rule_set.append(rule)\n",
    "        else:\n",
    "            p_count = semantic_structure.count(\"_p\")\n",
    "            x_count = semantic_structure.count(\"_x\")\n",
    "            y_count = semantic_structure.count(\"_y\")\n",
    "            total_variables = p_count + x_count + y_count\n",
    "\n",
    "            if total_variables == 0:\n",
    "                holistic_rule_set.append(rule)\n",
    "            elif total_variables == 1:\n",
    "                generalization_rule_set_1.append(rule)\n",
    "            elif total_variables == 2:\n",
    "                generalization_rule_set_2.append(rule)\n",
    "            elif total_variables == 3:\n",
    "                generalization_rule_set_3.append(rule)\n",
    "\n",
    "    return holistic_rule_set, generalization_rule_set_1, generalization_rule_set_2, generalization_rule_set_3, word_rule_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01380e11-df1b-4d18-911b-c5a15f1233f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S/_believe(_ibuki, _quolia)/0 -> affordance', 'S/_help(_alice, _blackawa)/0 -> communication', 'S/_follow(_McKirby, _Kalin)/0 -> autopoiesis']\n",
      "['S/_love(_x, _koko)/0 -> jazzΠ/x', 'S/_love(_x, _koko)/0 -> Π/xhiphop', 'S/_love(_ibuki, _y)/0 -> E/yhiphop']\n",
      "['S/_p(_hash, _y)/0 -> D/ybakabakkaW/p']\n",
      "[]\n",
      "['W/_kill -> iwrhtb', 'D/_ibuki -> pow', 'Π/_blackawa -> ljk']\n"
     ]
    }
   ],
   "source": [
    "holistic_rule_set, generalization_rule_set_1, generalization_rule_set_2, generalization_rule_set_3, word_rule_set = clustering_rule_set(rule_set)\n",
    "print(holistic_rule_set)\n",
    "print(generalization_rule_set_1)\n",
    "print(generalization_rule_set_2)\n",
    "print(generalization_rule_set_3)\n",
    "print(word_rule_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06b194b-143e-410a-8762-7d0a8ad54690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_pair_sem_form(rule_set):\n",
    "    pair_sem_form_set = []\n",
    "    for a_rule in rule_set:\n",
    "        a_pair_sem_form = parse_rule(a_rule)\n",
    "        pair_sem_form_set.append(a_pair_sem_form)\n",
    "    return pair_sem_form_set\n",
    "\n",
    "def set_pair_sem_form_for_word_rule(rule_set):\n",
    "    pair_sem_form_set = []\n",
    "    for a_rule in rule_set:\n",
    "        pair_sem_form_set.append(a_rule)\n",
    "    return pair_sem_form_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f27fc5-4fe7-4ee0-b147-b2ba41b3e5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('S/_believe(_ibuki, _quolia)/0', 'affordance'), ('S/_help(_alice, _blackawa)/0', 'communication'), ('S/_follow(_McKirby, _Kalin)/0', 'autopoiesis')]\n",
      "[('S/_love(_x, _koko)/0', 'jazzΠ/x'), ('S/_love(_x, _koko)/0', 'Π/xhiphop'), ('S/_love(_ibuki, _y)/0', 'E/yhiphop')]\n",
      "[('S/_p(_hash, _y)/0', 'D/ybakabakkaW/p')]\n",
      "[]\n",
      "['W/_kill -> iwrhtb', 'D/_ibuki -> pow', 'Π/_blackawa -> ljk']\n"
     ]
    }
   ],
   "source": [
    "holistic_rule_set, generalization_rule_set_1, generalization_rule_set_2, generalization_rule_set_3, word_rule_set = clustering_rule_set(rule_set)\n",
    "\n",
    "holistic_rule_set = set_pair_sem_form(holistic_rule_set)\n",
    "print(holistic_rule_set)\n",
    "variable_1_pair_sem_form_set = set_pair_sem_form(generalization_rule_set_1)\n",
    "print(variable_1_pair_sem_form_set)\n",
    "variable_2_pair_sem_form_set = set_pair_sem_form(generalization_rule_set_2)\n",
    "print(variable_2_pair_sem_form_set)\n",
    "variable_3_pair_sem_form_set = set_pair_sem_form(generalization_rule_set_3)\n",
    "print(variable_3_pair_sem_form_set)\n",
    "word_rule_set = set_pair_sem_form_for_word_rule(word_rule_set)\n",
    "print(word_rule_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e07664f-c97a-464a-8e6c-0c1806051f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_rule_sets(rule_set):\n",
    "    holistic_rule_set, generalization_rule_set_1, generalization_rule_set_2, generalization_rule_set_3, word_rule_set = clustering_rule_set(rule_set)\n",
    "\n",
    "    holistic_rule_set = set_pair_sem_form(holistic_rule_set)\n",
    "    variable_1_pair_sem_form_set = set_pair_sem_form(generalization_rule_set_1)\n",
    "    variable_2_pair_sem_form_set = set_pair_sem_form(generalization_rule_set_2)\n",
    "    variable_3_pair_sem_form_set = set_pair_sem_form(generalization_rule_set_3)\n",
    "    word_rule_set = set_pair_sem_form_for_word_rule(word_rule_set)\n",
    "\n",
    "    return holistic_rule_set, variable_1_pair_sem_form_set, variable_2_pair_sem_form_set, variable_3_pair_sem_form_set, word_rule_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88f85f2d-c144-483f-8a55-ab5aad09392c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('S/_believe(_ibuki, _quolia)/0', 'affordance'), ('S/_help(_alice, _blackawa)/0', 'communication'), ('S/_follow(_McKirby, _Kalin)/0', 'autopoiesis')]\n",
      "[('S/_love(_x, _koko)/0', 'jazzΠ/x'), ('S/_love(_x, _koko)/0', 'Π/xhiphop'), ('S/_love(_ibuki, _y)/0', 'E/yhiphop')]\n",
      "[('S/_p(_hash, _y)/0', 'D/ybakabakkaW/p')]\n",
      "[]\n",
      "['W/_kill -> iwrhtb', 'D/_ibuki -> pow', 'Π/_blackawa -> ljk']\n"
     ]
    }
   ],
   "source": [
    "holistic_rule_set, variable_1_pair_sem_form_set, variable_2_pair_sem_form_set, variable_3_pair_sem_form_set, word_rule_set = initialize_rule_sets(rule_set)\n",
    "print(holistic_rule_set)\n",
    "print(variable_1_pair_sem_form_set)\n",
    "print(variable_2_pair_sem_form_set)\n",
    "print(variable_3_pair_sem_form_set)\n",
    "print(word_rule_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d8979b1-320e-4128-abb1-68f76ed8dc9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_semantics_ability(semantic_elements):\n",
    "    return re.findall(r'_[a-zA-Z0-9]+|\\(\\w+\\)|[A-Z]+|/[0-9]', semantic_elements)\n",
    "\n",
    "def split_semantics_process_for_rule_set(semantic_set):\n",
    "    split_semantic_elements_set_in_rule_set = []\n",
    "    for a_semantic_element in semantic_set:\n",
    "        a_sem_express = a_semantic_element[0]\n",
    "        a_form_express = a_semantic_element[1]\n",
    "        split_semantics = split_semantics_ability(a_sem_express)\n",
    "        split_semantic_elements_set_in_rule_set.append([split_semantics, [a_form_express]])\n",
    "    return split_semantic_elements_set_in_rule_set\n",
    "\n",
    "def split_semantics_process(semantic_set):\n",
    "    split_semantic_elements_set = []\n",
    "    for a_semantic_element in semantic_set:\n",
    "        one_of_semantic_set = split_semantics_ability(a_semantic_element)\n",
    "        split_semantic_elements_set.append(one_of_semantic_set)\n",
    "    return split_semantic_elements_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf7bf13-314e-49ec-8b4e-fea83ce62855",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['S', '_believe', '_ibuki', '_quolia', '/0'], ['affordance']], [['S', '_help', '_alice', '_blackawa', '/0'], ['communication']], [['S', '_follow', '_McKirby', '_Kalin', '/0'], ['autopoiesis']]]\n",
      "[[['S', '_love', '_x', '_koko', '/0'], ['jazzΠ/x']], [['S', '_love', '_x', '_koko', '/0'], ['Π/xhiphop']], [['S', '_love', '_ibuki', '_y', '/0'], ['E/yhiphop']]]\n",
      "[[['S', '_p', '_hash', '_y', '/0'], ['D/ybakabakkaW/p']]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 全体論ルールの意味表現の分割\n",
    "split_semantic_elements_set_in_holistic_rule_set = split_semantics_process_for_rule_set(holistic_rule_set)\n",
    "print(split_semantic_elements_set_in_holistic_rule_set)\n",
    "# 変項1の意味表現の分割\n",
    "split_semantic_elements_set_in_generalization_rule_set_1 = split_semantics_process_for_rule_set(variable_1_pair_sem_form_set)\n",
    "print(split_semantic_elements_set_in_generalization_rule_set_1)\n",
    "# 変項2の意味表現の分割\n",
    "split_semantic_elements_set_in_generalization_rule_set_2 = split_semantics_process_for_rule_set(variable_2_pair_sem_form_set)\n",
    "print(split_semantic_elements_set_in_generalization_rule_set_2)\n",
    "# 変項3の意味表現の分割\n",
    "split_semantic_elements_set_in_generalization_rule_set_3 = split_semantics_process_for_rule_set(variable_3_pair_sem_form_set)\n",
    "print(split_semantic_elements_set_in_generalization_rule_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4633f68-ab6e-4b06-8c5b-3a39b5e626e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S', '_believe', '_ibuki', '_quolia', '/0'], ['S', '_help', '_alice', '_blackawa', '/0'], ['S', '_believe', '_blackawa', '_quolia', '/0'], ['S', '_help', '_ibuki', '_quolia', '/0'], ['S', '_love', '_ibuki', '_koko', '/0'], ['S', '_love', '_blackawa', '_koko', '/0'], ['S', '_kill', '_hash', '_ibuki', '/0'], ['S', '_kill', '_hash', '_koko', '/0'], ['S', '_kill', '_ibuki', '_blackawa', '/0'], ['S', '_kill', '_ibuki', '_koko', '/0'], ['S', '_help', '_ibuki', '_koko', '/0'], ['S', '_help', '_sakana', '_koko', '/0']]\n"
     ]
    }
   ],
   "source": [
    "# only_sem_express_set_for_productionの意味表現の分割\n",
    "split_semantic_elements_set_in_production = split_semantics_process(only_sem_express_set_for_production)\n",
    "print(split_semantic_elements_set_in_production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8a63cee-7ca8-4568-b0e0-fc3201e62d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_semantic_elements(rule_set):\n",
    "    holistic_rule_set, variable_1_pair_sem_form_set, variable_2_pair_sem_form_set, variable_3_pair_sem_form_set, word_rule_set = initialize_rule_sets(rule_set)\n",
    "\n",
    "    split_semantic_elements_set_in_holistic_rule_set = split_semantics_process_for_rule_set(holistic_rule_set)\n",
    "    split_semantic_elements_set_in_generalization_rule_set_1 = split_semantics_process_for_rule_set(variable_1_pair_sem_form_set)\n",
    "    split_semantic_elements_set_in_generalization_rule_set_2 = split_semantics_process_for_rule_set(variable_2_pair_sem_form_set)\n",
    "    split_semantic_elements_set_in_generalization_rule_set_3 = split_semantics_process_for_rule_set(variable_3_pair_sem_form_set)\n",
    "\n",
    "    return split_semantic_elements_set_in_holistic_rule_set, split_semantic_elements_set_in_generalization_rule_set_1, split_semantic_elements_set_in_generalization_rule_set_2, split_semantic_elements_set_in_generalization_rule_set_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "520c8fd7-416e-4d98-9d49-e69e9217f100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['S', '_believe', '_ibuki', '_quolia', '/0'], ['affordance']], [['S', '_help', '_alice', '_blackawa', '/0'], ['communication']], [['S', '_follow', '_McKirby', '_Kalin', '/0'], ['autopoiesis']]]\n",
      "[[['S', '_love', '_x', '_koko', '/0'], ['jazzΠ/x']], [['S', '_love', '_x', '_koko', '/0'], ['Π/xhiphop']], [['S', '_love', '_ibuki', '_y', '/0'], ['E/yhiphop']]]\n",
      "[[['S', '_p', '_hash', '_y', '/0'], ['D/ybakabakkaW/p']]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "split_semantic_elements_set_in_holistic_rule_set, split_semantic_elements_set_in_generalization_rule_set_1, split_semantic_elements_set_in_generalization_rule_set_2, split_semantic_elements_set_in_generalization_rule_set_3 = initialize_semantic_elements(rule_set)\n",
    "print(split_semantic_elements_set_in_holistic_rule_set)\n",
    "print(split_semantic_elements_set_in_generalization_rule_set_1)\n",
    "print(split_semantic_elements_set_in_generalization_rule_set_2)\n",
    "print(split_semantic_elements_set_in_generalization_rule_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "622e6352-a25d-45df-904f-e55810d13e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_sem_difference_ability(split_sem1, split_sem2):\n",
    "    sem_differences = 0\n",
    "    for sem_element1, sem_element2 in zip(split_sem1, split_sem2):\n",
    "        if sem_element1 != sem_element2:\n",
    "            sem_differences += 1\n",
    "    return sem_differences\n",
    "\n",
    "def compare_production_sem_with_any_rule_set_ability(a_split_semantic_elements_set_in_production, any_rule_set, allowed_variables):\n",
    "    variables = []\n",
    "    if not any_rule_set:\n",
    "        return variables\n",
    "    \n",
    "    for an_element_in_any_rule_set in any_rule_set:\n",
    "        number_of_variables = count_sem_difference_ability(a_split_semantic_elements_set_in_production, an_element_in_any_rule_set[0])\n",
    "        if number_of_variables == allowed_variables:\n",
    "            variables.append((an_element_in_any_rule_set, a_split_semantic_elements_set_in_production, number_of_variables))\n",
    "    return variables\n",
    "\n",
    "def pairing_production_and_rules_with_any_variables_process(\n",
    "    split_semantic_elements_set_in_production,\n",
    "    split_semantic_elements_set_in_holistic_rule_set,\n",
    "    split_semantic_elements_set_in_generalization_rule_set_1,\n",
    "    split_semantic_elements_set_in_generalization_rule_set_2,\n",
    "    split_semantic_elements_set_in_generalization_rule_set_3\n",
    "):\n",
    "    any_diff_in_sem_pairs = []\n",
    "    \n",
    "    for a_split_semantic_elements_set_in_production in split_semantic_elements_set_in_production:\n",
    "        \n",
    "        holistic_pairs = compare_production_sem_with_any_rule_set_ability(a_split_semantic_elements_set_in_production, split_semantic_elements_set_in_holistic_rule_set, 0)\n",
    "        if holistic_pairs:\n",
    "            any_diff_in_sem_pairs.append(holistic_pairs)\n",
    "            continue\n",
    "        \n",
    "        variable1_pairs = compare_production_sem_with_any_rule_set_ability(a_split_semantic_elements_set_in_production, split_semantic_elements_set_in_generalization_rule_set_1, 1)\n",
    "        if variable1_pairs:\n",
    "            any_diff_in_sem_pairs.append(variable1_pairs)\n",
    "            continue\n",
    "        \n",
    "        variable2_pairs = compare_production_sem_with_any_rule_set_ability(a_split_semantic_elements_set_in_production, split_semantic_elements_set_in_generalization_rule_set_2, 2)\n",
    "        if variable2_pairs:\n",
    "            any_diff_in_sem_pairs.append(variable2_pairs)\n",
    "            continue\n",
    "        \n",
    "        variable3_pairs = compare_production_sem_with_any_rule_set_ability(a_split_semantic_elements_set_in_production, split_semantic_elements_set_in_generalization_rule_set_3, 3)\n",
    "        if variable3_pairs:\n",
    "            any_diff_in_sem_pairs.append(variable3_pairs)\n",
    "            continue\n",
    "        \n",
    "        # print(a_split_semantic_elements_set_in_production)\n",
    "        any_diff_in_sem_pairs.append(a_split_semantic_elements_set_in_production)\n",
    "    \n",
    "    return any_diff_in_sem_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3f907a7-9e63-48fe-8993-4e0799dbf6da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[([['S', '_believe', '_ibuki', '_quolia', '/0'], ['affordance']], ['S', '_believe', '_ibuki', '_quolia', '/0'], 0)], [([['S', '_help', '_alice', '_blackawa', '/0'], ['communication']], ['S', '_help', '_alice', '_blackawa', '/0'], 0)], ['S', '_believe', '_blackawa', '_quolia', '/0'], ['S', '_help', '_ibuki', '_quolia', '/0'], [([['S', '_love', '_x', '_koko', '/0'], ['jazzΠ/x']], ['S', '_love', '_ibuki', '_koko', '/0'], 1), ([['S', '_love', '_x', '_koko', '/0'], ['Π/xhiphop']], ['S', '_love', '_ibuki', '_koko', '/0'], 1), ([['S', '_love', '_ibuki', '_y', '/0'], ['E/yhiphop']], ['S', '_love', '_ibuki', '_koko', '/0'], 1)], [([['S', '_love', '_x', '_koko', '/0'], ['jazzΠ/x']], ['S', '_love', '_blackawa', '_koko', '/0'], 1), ([['S', '_love', '_x', '_koko', '/0'], ['Π/xhiphop']], ['S', '_love', '_blackawa', '_koko', '/0'], 1)], [([['S', '_p', '_hash', '_y', '/0'], ['D/ybakabakkaW/p']], ['S', '_kill', '_hash', '_ibuki', '/0'], 2)], [([['S', '_p', '_hash', '_y', '/0'], ['D/ybakabakkaW/p']], ['S', '_kill', '_hash', '_koko', '/0'], 2)], ['S', '_kill', '_ibuki', '_blackawa', '/0'], ['S', '_kill', '_ibuki', '_koko', '/0'], ['S', '_help', '_ibuki', '_koko', '/0'], ['S', '_help', '_sakana', '_koko', '/0']]\n"
     ]
    }
   ],
   "source": [
    "any_diff_in_sem_pairs = pairing_production_and_rules_with_any_variables_process(\n",
    "    split_semantic_elements_set_in_production,\n",
    "    split_semantic_elements_set_in_holistic_rule_set,\n",
    "    split_semantic_elements_set_in_generalization_rule_set_1,\n",
    "    split_semantic_elements_set_in_generalization_rule_set_2,\n",
    "    split_semantic_elements_set_in_generalization_rule_set_3\n",
    ")\n",
    "print(any_diff_in_sem_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98cee5f8-fb01-4795-8e37-e78127c84ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "発話指令のみのリスト:\n",
      "[['S', '_believe', '_blackawa', '_quolia', '/0'], ['S', '_help', '_ibuki', '_quolia', '/0'], ['S', '_kill', '_ibuki', '_blackawa', '/0'], ['S', '_kill', '_ibuki', '_koko', '/0'], ['S', '_help', '_ibuki', '_koko', '/0'], ['S', '_help', '_sakana', '_koko', '/0']]\n",
      "\n",
      "発話指令以外のリスト:\n",
      "[[([['S', '_believe', '_ibuki', '_quolia', '/0'], ['affordance']], ['S', '_believe', '_ibuki', '_quolia', '/0'], 0)], [([['S', '_help', '_alice', '_blackawa', '/0'], ['communication']], ['S', '_help', '_alice', '_blackawa', '/0'], 0)], [([['S', '_love', '_x', '_koko', '/0'], ['jazzΠ/x']], ['S', '_love', '_ibuki', '_koko', '/0'], 1), ([['S', '_love', '_x', '_koko', '/0'], ['Π/xhiphop']], ['S', '_love', '_ibuki', '_koko', '/0'], 1), ([['S', '_love', '_ibuki', '_y', '/0'], ['E/yhiphop']], ['S', '_love', '_ibuki', '_koko', '/0'], 1)], [([['S', '_love', '_x', '_koko', '/0'], ['jazzΠ/x']], ['S', '_love', '_blackawa', '_koko', '/0'], 1), ([['S', '_love', '_x', '_koko', '/0'], ['Π/xhiphop']], ['S', '_love', '_blackawa', '_koko', '/0'], 1)], [([['S', '_p', '_hash', '_y', '/0'], ['D/ybakabakkaW/p']], ['S', '_kill', '_hash', '_ibuki', '/0'], 2)], [([['S', '_p', '_hash', '_y', '/0'], ['D/ybakabakkaW/p']], ['S', '_kill', '_hash', '_koko', '/0'], 2)]]\n"
     ]
    }
   ],
   "source": [
    "def cluster_any_diff_in_sem_pairs(any_diff_in_sem_pairs):\n",
    "    # 発話指令のみを格納するリスト\n",
    "    only_command_list = []\n",
    "    # 発話指令以外（3つの要素がある）のリスト\n",
    "    command_with_info_list = []\n",
    "    \n",
    "    for item in any_diff_in_sem_pairs:\n",
    "        # 発話指令だけの場合\n",
    "        if isinstance(item, list) and all(isinstance(elem, str) for elem in item):\n",
    "            only_command_list.append(item)\n",
    "        # 既存知識、発話指令、difference_countが含まれる場合\n",
    "        else:\n",
    "            command_with_info_list.append(item)\n",
    "    \n",
    "    return only_command_list, command_with_info_list\n",
    "\n",
    "# クラスタリングを実行\n",
    "only_command_list, command_with_info_list = cluster_any_diff_in_sem_pairs(any_diff_in_sem_pairs)\n",
    "\n",
    "# 結果の表示\n",
    "print(\"発話指令のみのリスト:\")\n",
    "print(only_command_list)\n",
    "print(\"\\n発話指令以外のリスト:\")\n",
    "print(command_with_info_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdb394-cd60-4d09-8a12-5f7c3b0c856d",
   "metadata": {},
   "source": [
    "まずは， word_rule_inventionを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5df9525c-780e-4984-b1cb-5a82cd41db3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def form_random_generation_ability(length): # 文全体ルールを作る\n",
    "    allowed_characters = ''.join(c for c in string.ascii_lowercase if c not in 'spxy')\n",
    "    return ''.join(random.choice(allowed_characters) for _ in range(length))\n",
    "\n",
    "def form_one_two_three_generation_ability(non_variable_number): # 単語ルールを作る\n",
    "    random_form = form_random_generation_ability(random.randint(1, 3))\n",
    "    return random_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d7f8796-b259-4884-b710-e6c2e97915c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ow\n"
     ]
    }
   ],
   "source": [
    "random_form = form_one_two_three_generation_ability(3)\n",
    "print(random_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e249784-b36a-44b9-8f85-c465765ba3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_invented_rules(utterance_list):\n",
    "    invented_rules_from_only_command_list = []\n",
    "\n",
    "    for utterance in utterance_list:\n",
    "        target_sem_express = '/'.join(utterance[:2])\n",
    "\n",
    "        if len(utterance) > 3:\n",
    "            target_sem_express += '(' + ','.join(utterance[2:-1]) + ')'  # 引数部分を追加\n",
    "        target_sem_express += utterance[-1]  # 最後の部分 (/0)を追加\n",
    "        \n",
    "        # 文全体をランダムに生成 (3〜9文字)\n",
    "        full_invention = form_random_generation_ability(random.randint(3, 9))\n",
    "        \n",
    "        # 発話指令に対して文全体の形式をランダムに適用\n",
    "        invented_rule = f\"{target_sem_express} -> {full_invention}\"\n",
    "        \n",
    "        # 生成したルールをリストに保存\n",
    "        invented_rules_from_only_command_list.append(invented_rule)\n",
    "    \n",
    "    return invented_rules_from_only_command_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3ed7d47-ae81-4f74-946e-fde90ebc87e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S/_believe(_blackawa,_quolia)/0 -> bchqt', 'S/_help(_ibuki,_quolia)/0 -> fiam', 'S/_kill(_ibuki,_blackawa)/0 -> ewu', 'S/_kill(_ibuki,_koko)/0 -> qrnk', 'S/_help(_ibuki,_koko)/0 -> jtvmm', 'S/_help(_sakana,_koko)/0 -> kewdaw']\n"
     ]
    }
   ],
   "source": [
    "# 文全体をランダムに生成\n",
    "invented_rules_from_only_command_list = generate_invented_rules(only_command_list)\n",
    "\n",
    "# 結果を表示\n",
    "print(invented_rules_from_only_command_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b087bd1-508d-43ad-bccd-fccb40dee72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_different_elements_and_format_ability(any_diff_in_sem_pair):\n",
    "#     list1, list2,_ = any_diff_in_sem_pair\n",
    "\n",
    "#     list1_flat = [item for sublist in list1 for item in sublist]\n",
    "#     list2_flat = list2\n",
    "\n",
    "#     differences = []\n",
    "#     formatted_results = []\n",
    "#     variable_and_sem_express_pairs = []\n",
    "\n",
    "#     for i in range(len(list2_flat)):\n",
    "#         if list1_flat[i] != list2_flat[i]:\n",
    "#             if not list1_flat[i].startswith('_'):\n",
    "#                 differences.append(list1_flat[i])\n",
    "#             differences.append(list2_flat[i])\n",
    "\n",
    "#             if list1_flat[i].startswith('_'):\n",
    "#                 variable = list1_flat[i][1]\n",
    "#                 for sublist in list1:\n",
    "#                     for item in sublist:\n",
    "#                         if variable in item:\n",
    "#                             index = item.index(variable)\n",
    "#                             if index >= 2:\n",
    "#                                 formatted_item = item[index-2:index] + variable\n",
    "#                                 formatted_results.append(formatted_item)\n",
    "                                \n",
    "#                                 formatted_combined = item[index-2:index] + list2_flat[i]\n",
    "#                                 variable_and_sem_express_pairs.append([variable, formatted_combined])\n",
    "\n",
    "#     final_result = formatted_results + differences\n",
    "#     return final_result, variable_and_sem_express_pairs\n",
    "\n",
    "\n",
    "def extract_different_elements_and_format_ability(any_diff_in_sem_pair):\n",
    "    list1, list2, _ = any_diff_in_sem_pair\n",
    "    list1_flat = list1[0]  # タプルの0番目のリストの最初のリスト\n",
    "    list2_flat = list2  # タプルの1番目のリスト\n",
    "    \n",
    "    differences = []\n",
    "    formatted_results = []\n",
    "    variable_and_sem_express_pairs = []\n",
    "\n",
    "    for i in range(len(list2_flat)):\n",
    "        if list1_flat[i] != list2_flat[i]:\n",
    "            if not list1_flat[i].startswith('_'):\n",
    "                differences.append(list1_flat[i])\n",
    "            differences.append(list2_flat[i])\n",
    "\n",
    "            if list1_flat[i].startswith('_'):\n",
    "                variable = list1_flat[i][1]\n",
    "                for sublist in list1:\n",
    "                    for item in sublist:\n",
    "                        if variable in item:\n",
    "                            index = item.index(variable)\n",
    "                            if index >= 2:\n",
    "                                formatted_item = item[index-2:index] + variable\n",
    "                                formatted_results.append(formatted_item)\n",
    "                                \n",
    "                                formatted_combined = item[index-2:index] + list2_flat[i]\n",
    "                                variable_and_sem_express_pairs.append([variable, formatted_combined])\n",
    "\n",
    "    final_result = formatted_results + differences\n",
    "\n",
    "    return final_result, variable_and_sem_express_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6a3c334-5c87-4d91-8f09-2e0077c1b0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result: ['Η/x', 'M/y', '_eve', '_alice']\n",
      "Variable and Semantic Expression Pairs: [['x', 'Η/_eve'], ['y', 'M/_alice']]\n"
     ]
    }
   ],
   "source": [
    "any_diff_in_sem_pair = [[([['S', '_admire', '_x', '_y', '/0'], ['ufΗ/xozvfbM/y']], ['S', '_admire', '_eve', '_alice', '/0'], 2)]]\n",
    "final_result, variable_and_sem_express_pairs = extract_different_elements_and_format_ability(any_diff_in_sem_pair[0][0])\n",
    "\n",
    "# 結果の表示\n",
    "print(\"Final Result:\", final_result)\n",
    "print(\"Variable and Semantic Expression Pairs:\", variable_and_sem_express_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff3978af-75d6-4915-809e-5bcf185c09c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_and_invention_word_rule_ability(variable_and_sem_express_pairs, word_rule_set):\n",
    "    word_variable_and_sem_express_pairs = []\n",
    "    invented_rules = []\n",
    "    generated_forms = {}\n",
    "\n",
    "    # デバッグ用に入力の内容を表示\n",
    "    print(\"Input variable_and_sem_express_pairs:\", variable_and_sem_express_pairs)\n",
    "    print(\"Input word_rule_set:\", word_rule_set)\n",
    "    \n",
    "    for pair in variable_and_sem_express_pairs:\n",
    "        matching_rules = []\n",
    "        target_sem_express = pair[1]\n",
    "        \n",
    "        # デバッグ：ペアを表示\n",
    "        print(f\"\\nProcessing pair: {pair}\")\n",
    "        print(f\"Target semantic expression: {target_sem_express}\")\n",
    "        \n",
    "        if target_sem_express in generated_forms:\n",
    "            selected_rule = generated_forms[target_sem_express]\n",
    "            # デバッグ：既に生成されたルールがある場合\n",
    "            print(f\"Found in generated_forms: {selected_rule}\")\n",
    "        else:\n",
    "            for rule in word_rule_set:\n",
    "                sem_express_in_rule = rule.split('->')[0]\n",
    "                # デバッグ：各ルールのセマンティック表現を表示\n",
    "                print(f\"Checking rule: {rule}, Semantic Expression in rule: {sem_express_in_rule}\")\n",
    "                if sem_express_in_rule == target_sem_express:\n",
    "                    matching_rules.append(rule)\n",
    "                    print(f\"ほい: {sem_express_in_rule}\")\n",
    "                    print(f\"だ: {target_sem_express}\")\n",
    "\n",
    "            # デバッグ：マッチしたルールを表示\n",
    "            print(f\"Matching rules: {matching_rules}\")\n",
    "            \n",
    "            if len(matching_rules) == 1:\n",
    "                selected_rule = matching_rules[0]\n",
    "                print(f\"Selected single matching rule: {selected_rule}\")\n",
    "            elif len(matching_rules) > 1:\n",
    "                selected_rule = random.choice(matching_rules)\n",
    "                print(f\"Multiple matching rules, selected: {selected_rule}\")\n",
    "            else:\n",
    "                random_word_form = form_one_two_three_generation_ability(random.randint(1, 3))\n",
    "                selected_rule = f\"{target_sem_express}->{random_word_form}\"\n",
    "                invented_rules.append(selected_rule)\n",
    "                generated_forms[target_sem_express] = selected_rule\n",
    "                print(f\"No matching rules, invented rule: {selected_rule}\")\n",
    "\n",
    "        word_variable_and_sem_express_pairs.append([pair[0], selected_rule])\n",
    "        # デバッグ：生成されたペアを表示\n",
    "        print(f\"Generated pair: {[pair[0], selected_rule]}\")\n",
    "    \n",
    "    # デバッグ：最終的な結果を表示\n",
    "    print(\"\\nFinal word_variable_and_sem_express_pairs:\", word_variable_and_sem_express_pairs)\n",
    "    print(\"Invented rules:\", invented_rules)\n",
    "\n",
    "    return word_variable_and_sem_express_pairs, invented_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61c9a4bf-556d-419a-95c6-373cae9c2ff9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input variable_and_sem_express_pairs: [['x', 'Η/_eve'], ['y', 'M/_alice']]\n",
      "Input word_rule_set: ['M/_alice->adf', 'A/_de->f', 'M/_alice->gh']\n",
      "\n",
      "Processing pair: ['x', 'Η/_eve']\n",
      "Target semantic expression: Η/_eve\n",
      "Checking rule: M/_alice->adf, Semantic Expression in rule: M/_alice\n",
      "Checking rule: A/_de->f, Semantic Expression in rule: A/_de\n",
      "Checking rule: M/_alice->gh, Semantic Expression in rule: M/_alice\n",
      "Matching rules: []\n",
      "No matching rules, invented rule: Η/_eve->vh\n",
      "Generated pair: ['x', 'Η/_eve->vh']\n",
      "\n",
      "Processing pair: ['y', 'M/_alice']\n",
      "Target semantic expression: M/_alice\n",
      "Checking rule: M/_alice->adf, Semantic Expression in rule: M/_alice\n",
      "ほい: M/_alice\n",
      "だ: M/_alice\n",
      "Checking rule: A/_de->f, Semantic Expression in rule: A/_de\n",
      "Checking rule: M/_alice->gh, Semantic Expression in rule: M/_alice\n",
      "ほい: M/_alice\n",
      "だ: M/_alice\n",
      "Matching rules: ['M/_alice->adf', 'M/_alice->gh']\n",
      "Multiple matching rules, selected: M/_alice->gh\n",
      "Generated pair: ['y', 'M/_alice->gh']\n",
      "\n",
      "Final word_variable_and_sem_express_pairs: [['x', 'Η/_eve->vh'], ['y', 'M/_alice->gh']]\n",
      "Invented rules: ['Η/_eve->vh']\n",
      "Word Variable and Semantic Expression Pairs: [['x', 'Η/_eve->vh'], ['y', 'M/_alice->gh']]\n",
      "Invented Rules: ['Η/_eve->vh']\n"
     ]
    }
   ],
   "source": [
    "variable_and_sem_express_pairs = [['x', 'Η/_eve'], ['y', 'M/_alice']]\n",
    "word_rule_set = ['M/_alice->adf', 'A/_de->f', 'M/_alice->gh']\n",
    "\n",
    "word_variable_and_sem_express_pairs, invented_rules = detect_and_invention_word_rule_ability(variable_and_sem_express_pairs, word_rule_set)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"Word Variable and Semantic Expression Pairs:\", word_variable_and_sem_express_pairs)\n",
    "print(\"Invented Rules:\", invented_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bd60f02-f34f-4fde-a649-661bd4be8d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_single_diff_in_sem_pair_ability(any_diff_in_sem_pair, word_rule_set):\n",
    "    list1, list2, difference_count = any_diff_in_sem_pair\n",
    "\n",
    "    if difference_count == 0:\n",
    "        return list1, [], 0, [], any_diff_in_sem_pair\n",
    "\n",
    "    final_result, variable_and_sem_express_pairs = extract_different_elements_and_format_ability(any_diff_in_sem_pair[0][0])\n",
    "    word_variable_and_sem_express_pairs, invented_rules = detect_and_invention_word_rule_ability(variable_and_sem_express_pairs, word_rule_set)\n",
    "\n",
    "    if len(invented_rules) >= 2:\n",
    "        full_invention = form_random_generation_ability(random.randint(3, 9))\n",
    "        final_result = [full_invention]\n",
    "        word_variable_and_sem_express_pairs = [[var, full_invention] for var,_ in word_variable_and_sem_express_pairs]\n",
    "        invented_rules = []\n",
    "        invention_count = 3\n",
    "    else:\n",
    "        invention_count = len(invented_rules)\n",
    "    \n",
    "    return final_result, word_variable_and_sem_express_pairs, invention_count, invented_rules, any_diff_in_sem_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0b60e6d-1fc3-44c7-91c5-872f9bcde94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_utterance(possible_pairs, word_rule_set):\n",
    "    best_pair = None\n",
    "    lowest_invented_rule_count = float('inf')\n",
    "    best_invented_rules = []\n",
    "    best_word_variable_and_sem_express_pairs = []\n",
    "\n",
    "    for i, pair in enumerate(possible_pairs):\n",
    "        final_result, word_variable_and_sem_express_pairs, invention_count, invented_rules,_ = process_single_diff_in_sem_pair_ability(pair, word_rule_set)\n",
    "\n",
    "        if invention_count < lowest_invented_rule_count:\n",
    "            lowest_invented_rule_count = invention_count\n",
    "            best_pair = pair\n",
    "            best_invented_rules = invented_rules\n",
    "            best_word_variable_and_sem_express_pairs = word_variable_and_sem_express_pairs\n",
    "        elif invention_count == lowest_invented_rule_count:\n",
    "            if random.choice([True, False]):\n",
    "                best_pair = pair\n",
    "                best_invented_rules = invented_rules\n",
    "                best_word_variable_and_sem_express_pairs = word_variable_and_sem_express_pairs\n",
    "\n",
    "    return best_pair, best_invented_rules, best_word_variable_and_sem_express_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0315c148-d483-4ffc-9971-f51d072e4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_return_production_ingredients(any_diff_in_sem_pairs, word_rule_set):\n",
    "    production_ingredients = []\n",
    "\n",
    "    for any_diff_in_sem_pair_group in any_diff_in_sem_pairs:\n",
    "        if isinstance(any_diff_in_sem_pair_group, list) and len(any_diff_in_sem_pair_group) > 1:\n",
    "            best_pair, best_invented_rules, best_word_variable_and_sem_express_pairs = select_best_utterance(any_diff_in_sem_pair_group, word_rule_set)\n",
    "            final_result,_,_,_,_ = process_single_diff_in_sem_pair_ability(best_pair, word_rule_set)\n",
    "            \n",
    "            if best_invented_rules:\n",
    "                word_rule_set.extend(best_invented_rules)\n",
    "            \n",
    "            production_ingredient = [best_pair, best_word_variable_and_sem_express_pairs]\n",
    "            production_ingredients.append(production_ingredient)\n",
    "        else:\n",
    "            final_result, best_word_variable_and_sem_express_pairs,_, best_invented_rules, best_pair = process_single_diff_in_sem_pair_ability(any_diff_in_sem_pair_group[0], word_rule_set)\n",
    "            \n",
    "            if best_invented_rules:\n",
    "                word_rule_set.extend(best_invented_rules)\n",
    "            \n",
    "            production_ingredient = [best_pair, best_word_variable_and_sem_express_pairs]\n",
    "            production_ingredients.append(production_ingredient)\n",
    "\n",
    "    return production_ingredients\n",
    "\n",
    "def combine_word_and_variable_ability(an_only_one_diff_in_sem_pair, word_variable_and_sem_express_pairs):\n",
    "    meaning_part = an_only_one_diff_in_sem_pair[1]\n",
    "    form_part = an_only_one_diff_in_sem_pair[0][1]\n",
    "    new_meaning = meaning_part[:]\n",
    "    new_form = ''.join(form_part)\n",
    "    \n",
    "    if len(word_variable_and_sem_express_pairs) == 3 and all(pair[1] == word_variable_and_sem_express_pairs[0][1] for pair in word_variable_and_sem_express_pairs):\n",
    "        word_form = word_variable_and_sem_express_pairs[0][1]\n",
    "        return restore_sentence_rule_ability([new_meaning, [word_form]])\n",
    "\n",
    "    for i, element in enumerate(new_meaning):\n",
    "        if element.startswith('_'):\n",
    "            position = element[1]\n",
    "\n",
    "            for var_pos, word_rule in word_variable_and_sem_express_pairs:\n",
    "                if var_pos == position:\n",
    "                    word_meaning, word_form = word_rule.split(' -> ')\n",
    "                    new_meaning[i] = word_meaning.split('/')[1]\n",
    "\n",
    "    for var_pos, word_rule in word_variable_and_sem_express_pairs:\n",
    "        category_label_and_sem_express, word_meaning_form = word_rule.split(' -> ')\n",
    "        word_form = word_meaning_form\n",
    "        category_label = category_label_and_sem_express.split('/')[0]\n",
    "        \n",
    "        new_form = new_form.replace(f'{category_label}/{var_pos}', word_form)\n",
    "\n",
    "    return restore_sentence_rule_ability([new_meaning, [new_form]])\n",
    "\n",
    "\n",
    "def restore_sentence_rule_ability(split_sentence_rule):\n",
    "    split_sem_express = split_sentence_rule[0]\n",
    "    \n",
    "    restored_sem_express = split_sem_express[0] + '/' + split_sem_express[1]\n",
    "\n",
    "    if len(split_sem_express) > 3:\n",
    "        restored_sem_express += '(' + ','.join(split_sem_express[2:-1]) + ')'\n",
    "    \n",
    "    restored_sem_express += split_sem_express[-1]\n",
    "    \n",
    "    form_express = split_sentence_rule[1][0]\n",
    "    \n",
    "    return f\"{restored_sem_express} -> {form_express}\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_sentence_rule(best_pair, word_variable_and_sem_express_pairs):\n",
    "    if not word_variable_and_sem_express_pairs: # 空　である\n",
    "        meaning_part = best_pair[1]\n",
    "        form_part = best_pair[0][1]\n",
    "        return f\"{meaning_part[0]}/\" + f\"{meaning_part[1]}\" + \"(\" + \",\".join(meaning_part[2:-1]) + \")\" + meaning_part[-1] + \" -> \" + \"\".join(form_part)\n",
    "    else: # 空　でない\n",
    "        return combine_word_and_variable_ability(best_pair, word_variable_and_sem_express_pairs)\n",
    "\n",
    "def generate_all_sentence_rules(production_ingredients):\n",
    "    generated_rules = []\n",
    "    \n",
    "    for pair_and_vars in production_ingredients:\n",
    "        best_pair, word_variable_and_sem_express_pairs = pair_and_vars\n",
    "        rule = generate_sentence_rule(best_pair, word_variable_and_sem_express_pairs)\n",
    "        generated_rules.append(rule)\n",
    "    \n",
    "    return generated_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "534b4785-21f0-4315-a7b0-fe183c56c7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def produce(rule_set, only_sem_express_set_for_production):\n",
    "    # 初期化\n",
    "    holistic_rule_set, variable_1_pair_sem_form_set, variable_2_pair_sem_form_set, variable_3_pair_sem_form_set, word_rule_set = initialize_rule_sets(rule_set)\n",
    "\n",
    "    split_semantic_elements_set_in_holistic_rule_set = split_semantics_process_for_rule_set(holistic_rule_set)\n",
    "    split_semantic_elements_set_in_generalization_rule_set_1 = split_semantics_process_for_rule_set(variable_1_pair_sem_form_set)\n",
    "    split_semantic_elements_set_in_generalization_rule_set_2 = split_semantics_process_for_rule_set(variable_2_pair_sem_form_set)\n",
    "    split_semantic_elements_set_in_generalization_rule_set_3 = split_semantics_process_for_rule_set(variable_3_pair_sem_form_set)\n",
    "    \n",
    "    split_semantic_elements_set_in_production = split_semantics_process(only_sem_express_set_for_production)\n",
    "\n",
    "    # 発話指令とルールの対応付け\n",
    "    any_diff_in_sem_pairs = pairing_production_and_rules_with_any_variables_process(\n",
    "        split_semantic_elements_set_in_production,\n",
    "        split_semantic_elements_set_in_holistic_rule_set,\n",
    "        split_semantic_elements_set_in_generalization_rule_set_1,\n",
    "        split_semantic_elements_set_in_generalization_rule_set_2,\n",
    "        split_semantic_elements_set_in_generalization_rule_set_3\n",
    "    )\n",
    "\n",
    "    only_command_list, command_with_info_list = cluster_any_diff_in_sem_pairs(any_diff_in_sem_pairs)\n",
    "    \n",
    "    \n",
    "    # 発話成分の生成\n",
    "    invented_rules_from_only_command_list = generate_invented_rules(only_command_list)\n",
    "    production_ingredients = process_and_return_production_ingredients(command_with_info_list, word_rule_set)\n",
    "    \n",
    "\n",
    "    # 生成された文規則\n",
    "    generated_rules = generate_all_sentence_rules(production_ingredients)\n",
    "    generated_rules.extend(invented_rules_from_only_command_list)\n",
    "\n",
    "    return generated_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "006caa41-e6af-49f4-b10d-fc32e89dffbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_rules \u001b[38;5;241m=\u001b[39m produce(rule_set, only_sem_express_set_for_production)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_rules)\n",
      "Cell \u001b[0;32mIn[31], line 30\u001b[0m, in \u001b[0;36mproduce\u001b[0;34m(rule_set, only_sem_express_set_for_production)\u001b[0m\n\u001b[1;32m     26\u001b[0m production_ingredients \u001b[38;5;241m=\u001b[39m process_and_return_production_ingredients(command_with_info_list, word_rule_set)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 生成された文規則\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m generated_rules \u001b[38;5;241m=\u001b[39m generate_all_sentence_rules(production_ingredients)\n\u001b[1;32m     31\u001b[0m generated_rules\u001b[38;5;241m.\u001b[39mextend(invented_rules_from_only_command_list)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_rules\n",
      "Cell \u001b[0;32mIn[30], line 83\u001b[0m, in \u001b[0;36mgenerate_all_sentence_rules\u001b[0;34m(production_ingredients)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair_and_vars \u001b[38;5;129;01min\u001b[39;00m production_ingredients:\n\u001b[1;32m     82\u001b[0m     best_pair, word_variable_and_sem_express_pairs \u001b[38;5;241m=\u001b[39m pair_and_vars\n\u001b[0;32m---> 83\u001b[0m     rule \u001b[38;5;241m=\u001b[39m generate_sentence_rule(best_pair, word_variable_and_sem_express_pairs)\n\u001b[1;32m     84\u001b[0m     generated_rules\u001b[38;5;241m.\u001b[39mappend(rule)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_rules\n",
      "Cell \u001b[0;32mIn[30], line 76\u001b[0m, in \u001b[0;36mgenerate_sentence_rule\u001b[0;34m(best_pair, word_variable_and_sem_express_pairs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeaning_part[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeaning_part[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(meaning_part[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m meaning_part[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(form_part)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combine_word_and_variable_ability(best_pair, word_variable_and_sem_express_pairs)\n",
      "Cell \u001b[0;32mIn[30], line 45\u001b[0m, in \u001b[0;36mcombine_word_and_variable_ability\u001b[0;34m(an_only_one_diff_in_sem_pair, word_variable_and_sem_express_pairs)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 new_meaning[i] \u001b[38;5;241m=\u001b[39m word_meaning\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var_pos, word_rule \u001b[38;5;129;01min\u001b[39;00m word_variable_and_sem_express_pairs:\n\u001b[0;32m---> 45\u001b[0m     category_label_and_sem_express, word_meaning_form \u001b[38;5;241m=\u001b[39m word_rule\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m     word_form \u001b[38;5;241m=\u001b[39m word_meaning_form\n\u001b[1;32m     47\u001b[0m     category_label \u001b[38;5;241m=\u001b[39m category_label_and_sem_express\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "generated_rules = produce(rule_set, only_sem_express_set_for_production)\n",
    "print(generated_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
